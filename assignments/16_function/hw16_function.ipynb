{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de7bac8",
   "metadata": {},
   "source": [
    "# Assigning function to reads\n",
    "\n",
    "This notebook will go through the workflow for assigning function to reads in a microbiome using HUMAnN 3.\n",
    "\n",
    "-----------\n",
    "\n",
    "Sections:\n",
    "\n",
    "1. Assign function to reads using HUMAnN 3.\n",
    "2. Summarize the HUMAnN 3 for KEGG terms \n",
    "\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebed36",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Set the variables you need for running the analyses in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a46521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the variables for your netid and xfile\n",
    "netid = \"MY_NETID\"\n",
    "xfile = \"MY_XFILE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d388ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into the working directory\n",
    "work_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/assignments/16_function\"\n",
    "%cd $work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the fastq directory. This is where we have our fastq files with human contam removed.\n",
    "fastq_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/assignments/07_contam_removal\"\n",
    "xfile_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/assignments/05_getting_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380dffed",
   "metadata": {},
   "source": [
    "## Creating a config file\n",
    "The scripts below executes code that requires certain variables to be set. So we don't need to edit the code in the script, we are going to use a config file that defines all of these variables for us. Then when we want to use these variables in the script, we will \"source\" the config file to set the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cf341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config file with all of the variables you need\n",
    "!echo \"export NETID=$netid\" > config.sh\n",
    "!echo \"export XFILE=$xfile\" >> config.sh\n",
    "!echo \"export WORK_DIR=$work_dir\" >> config.sh\n",
    "!echo \"export FASTQ_DIR=$fastq_dir\" >> config.sh\n",
    "!echo \"export XFILE_DIR=$xfile_dir\" >> config.sh\n",
    "!echo \"export HU3_DB=/xdisk/bhurwitz/databases/Humann3\" >> config.sh\n",
    "!echo \"export MPA_DB=/xdisk/bhurwitz/databases/Metaphlan\" >> config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the config file to be sure it is correct\n",
    "# Is your netid and xfile correct? Do you have the right directories?\n",
    "!cat config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeaabfe",
   "metadata": {},
   "source": [
    "## Step 1: Running HUMANN3 to get functional potential for the reads\n",
    "\n",
    "In this step, we will compare all of our trimmed/screened reads to the functional databases using HUMAnN. \n",
    "\n",
    "HUMAnN is a computational tool used for metagenomic functional profiling. It is part of the HUMAnN (HMP Unified Metabolic Analysis Network) family of tools and is designed to help researchers analyze and interpret the functional potential of microbial communities (metagenomes) derived from DNA sequencing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a run script to run HUMAnN3\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=24:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-7                         \n",
    "#SBATCH --output=16A_function-%a.out\n",
    "#SBATCH --error=16A_function-%a.err\n",
    "#SBATCH --cpus-per-task=28\n",
    "#SBATCH --mem-per-cpu=6G                                    \n",
    "\n",
    "pwd; hostname; date\n",
    "source $SLURM_SUBMIT_DIR/config.sh\n",
    "\n",
    "#load environment\n",
    "CONDA=\"/groups/bhurwitz/miniconda3\"\n",
    "source $CONDA/etc/profile.d/conda.sh\n",
    "conda activate humann3_env \n",
    "\n",
    "names=($(cat $XFILE_DIR/$XFILE))\n",
    "SAMPLE_ID=${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "ZPAIR1=${FASTQ_DIR}/${SAMPLE_ID}_1.fastq.gz\n",
    "gunzip ${ZPAIR1}\n",
    "PAIR1=${FASTQ_DIR}/${SAMPLE_ID}_1.fastq\n",
    "\n",
    "#No PAIR2 needed\n",
    "#PAIR2=${FASTQ_DIR}/${SAMPLE_ID}_2.fastq.gz\n",
    "\n",
    "OUT_DIR=\"$WORK_DIR/out_humann3/${SAMPLE_ID}\"\n",
    "\n",
    "if [[ ! -d \"$OUT_DIR\" ]]; then\n",
    "        mkdir -p $OUT_DIR\n",
    "fi\n",
    "\n",
    "echo ${PAIR1}\n",
    "\n",
    "#run humann\n",
    "humann --input ${PAIR1} --input-format fastq \\\n",
    "    -o ${OUT_DIR} \\\n",
    "    --metaphlan-options=\"-t rel_ab --bowtie2db ${MPA_DB} --index mpa_vJun23_CHOCOPhlAnSGB_202403\" \\\n",
    "    --nucleotide-database ${HU3_DB}/chocophlan --search-mode uniref90 \\\n",
    "    --protein-database ${HU3_DB}/uniref --threads 28\n",
    "\n",
    "gzip ${PAIR1}\n",
    "\n",
    "'''\n",
    "\n",
    "with open('16A_read_function.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e42306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should be in your working directory when you run this script\n",
    "# do you see your config.sh file, and the 16A_function.sh script?\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c692ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=24:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --output=16B_merge_humann3-%a.out\n",
    "#SBATCH --error=16B_merge_humann3%a.err\n",
    "#SBATCH --array=0-7 \n",
    "#SBATCH --cpus-per-task=5\n",
    "#SBATCH --mem-per-cpu=6G                                    \n",
    "\n",
    "pwd; hostname; date\n",
    "source $SLURM_SUBMIT_DIR/config.sh\n",
    "\n",
    "names=($(cat $XFILE_DIR/$XFILE))\n",
    "SAMPLE_ID=${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "#load environment\n",
    "CONDA=\"/groups/bhurwitz/miniconda3\"\n",
    "source $CONDA/etc/profile.d/conda.sh\n",
    "conda activate humann3_env \n",
    "\n",
    "IN_DIR=\"$WORK_DIR/out_humann3/${SAMPLE_ID}\"\n",
    "OUT_DIR=\"$WORK_DIR/out_pathabundance/${SAMPLE_ID}\"\n",
    "\n",
    "if [[ ! -d \"$OUT_DIR\" ]]; then\n",
    "        mkdir -p $OUT_DIR\n",
    "fi\n",
    "\n",
    "cd $IN_DIR\n",
    "\n",
    "# normalize path abundance\n",
    "for f in *_pathabundance.tsv\n",
    "do\n",
    "    humann_renorm_table --input $f --output cpm_$f --units cpm\n",
    "    mv cpm_$f $OUT_DIR\n",
    "done\n",
    "\n",
    "# merge table and stratify\n",
    "\n",
    "mkdir result_tables\n",
    "humann_join_tables --input $OUT_DIR --output humann_pathabundance.tsv --file_name cpm_\n",
    "humann_split_stratified_table --input humann_pathabundance.tsv --output result_tables\n",
    "mv humann_pathabundance.tsv result_tables\n",
    "\n",
    "OUT_DIR=\"$WORK_DIR/out_geneabundance/${SAMPLE_ID}\"\n",
    "\n",
    "cd $IN_DIR\n",
    "\n",
    "if [[ ! -d \"$OUT_DIR\" ]]; then\n",
    "        mkdir -p $OUT_DIR\n",
    "fi\n",
    "\n",
    "# normalize gene abundance and group by KEGG\n",
    "for f in *_genefamilies.tsv\n",
    "do\n",
    "    Kf=\"${f%%.tsv}KEGG.tsv\"\n",
    "    humann_regroup_table --input $f --output $Kf --custom /xdisk/bhurwitz/databases/Humann3/utility_mapping/map_ko_uniref90.txt.gz\n",
    "    humann_renorm_table --input $Kf --output cpm_$Kf --units cpm\n",
    "    mv cpm_$Kf $OUT_DIR\n",
    "done\n",
    "\n",
    "# merge table and stratify\n",
    "\n",
    "humann_join_tables --input $OUT_DIR --output humann_KOabundance.tsv --file_name cpm_\n",
    "humann_split_stratified_table --input humann_KOabundance.tsv --output result_tables\n",
    "mv humann_KOabundance.tsv result_tables\n",
    "\n",
    "# all without norm\n",
    "humann_join_tables --input . --output humann_KOnonnorm.tsv --file_name _genefamiliesKEGG.tsv\n",
    "humann_split_stratified_table --input humann_KOnonnorm.tsv --output result_tables_nonnorm\n",
    "mv humann_KOnonnorm.tsv result_tables_nonnorm\n",
    "\n",
    "'''\n",
    "\n",
    "with open('16B_merge_humann3.sh', mode='w') as file:\n",
    "    file.write(my_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480dc38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the launcher script to kick off our pipeline.\n",
    "\n",
    "my_code = '''#! /bin/bash\n",
    "\n",
    "# 16A_read_function: first job - no dependencies\n",
    "job1=$(sbatch 16A_read_function.sh)\n",
    "jid1=$(echo $job1 | sed 's/^Submitted batch job //')\n",
    "echo $jid1\n",
    "\n",
    "# 06B_run_trimmomatic: jid2 depends on jid1\n",
    "job2=$(sbatch --dependency=afterok:$jid1 16B_merge_humann3.sh)\n",
    "jid2=$(echo $job2 | sed 's/^Submitted batch job //')\n",
    "echo $jid2\n",
    "\n",
    "'''\n",
    "\n",
    "with open('16_launch_pipeline.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the pipeline script executable\n",
    "!chmod +x *.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e36574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's run it!\n",
    "!./16_launch_pipeline.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if it is running using the squeue command\n",
    "# Check for all jobs under your netid\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b30b51",
   "metadata": {},
   "source": [
    "### Time to wait...\n",
    "\n",
    "Great job! You kicked off a script to get functional annoation for your data. Now, you need to wait for this to complete. It should take an hour to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83501ab5",
   "metadata": {},
   "source": [
    "## Final Step\n",
    "Copy your notebook to the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ~/be487-fall-2024/assignments/16_function/hw16_function.ipynb $work_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
