{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5de7bac8",
   "metadata": {},
   "source": [
    "# Taxonomic Annotation of Reads\n",
    "\n",
    "This notebook will go through the workflow assigning taxonomy to reads from a microbiome\n",
    "\n",
    "Step 1: Assign taxonomy to reads (using 3 parts that run in a single job)\n",
    "1. Taxonomic assignment of reads using Kraken2\n",
    "2. Refinement of the taxonomic annotation using Bracken\n",
    "3. Separation of human and non-human reads using KrakenTools\n",
    "\n",
    "Step 2: Assign taxonomy to contigs from megahit (with 3 parts as above)\n",
    "\n",
    "Step 3: Assign taxonomy to contigs from metaspades (with 3 parts as above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebed36",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "You will need to rerun this section each time you come back to this notebook to reset all directories and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a46521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the variables for your netid and xfile\n",
    "netid = \"MY_NETID\"\n",
    "xfile = \"MY_XFILE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d388ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into the working directory\n",
    "work_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/assignments/11_taxonomy\"\n",
    "%cd $work_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380dffed",
   "metadata": {},
   "source": [
    "## Creating a config file\n",
    "Let's create a config file with all of the variables we will need in the scripts below. Then when we want to use these variables in the script, we will \"source\" the config file to set the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cf341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config file with all of the variables you need\n",
    "# notice that now we are defining the tools here too, to make out scripts easier to read below\n",
    "# also, if we want to update to a newer version of the tool we can just edit here.\n",
    "!echo \"export NETID=$netid\" > config.sh\n",
    "!echo \"export XFILE=$xfile\" >> config.sh\n",
    "!echo \"export WORK_DIR=/xdisk/bhurwitz/bh_class/$netid/assignments/11_taxonomy\" >> config.sh\n",
    "!echo \"export XFILE_DIR=/xdisk/bhurwitz/bh_class/$netid/assignments/05_getting_data\" >> config.sh\n",
    "!echo \"export FASTQ_DIR=/xdisk/bhurwitz/bh_class/$netid/assignments/07_contam_removal\" >> config.sh\n",
    "!echo \"export MEGAHIT_CONTIG_DIR=/xdisk/bhurwitz/bh_class/$netid/assignments/08_assembly/out_megahit\" >> config.sh\n",
    "!echo \"export METASPADES_CONTIG_DIR=/xdisk/bhurwitz/bh_class/$netid/assignments/08_assembly/out_spades\" >> config.sh\n",
    "!echo \"export CONTAINERS=/contrib/singularity/shared/bhurwitz\" >> config.sh\n",
    "!echo \"export KRAKEN2=/contrib/singularity/shared/bhurwitz/kraken2:2.1.3--pl5321hdcf5f25_0.sif\" >> config.sh\n",
    "!echo \"export BRACKEN=/contrib/singularity/shared/bhurwitz/bracken:2.8--py39h1f90b4d_1.sif\" >> config.sh\n",
    "!echo \"export KRAKENTOOLS=/contrib/singularity/shared/bhurwitz/krakentools:1.2--pyh5e36f6f_0.sif\" >> config.sh\n",
    "!echo \"export DB_DIR=/groups/bhurwitz/databases/kraken2/k2_pluspf_20230605\" >> config.sh\n",
    "!echo \"export KMER_SIZE=100\" >> config.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the config file to be sure it is correct\n",
    "# Is your netid and xfile correct? Do you have the right directories?\n",
    "!cat config.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfeaabfe",
   "metadata": {},
   "source": [
    "## Step 1: Assigning taxonomy to the reads\n",
    "\n",
    "In this step, we will assign taxonomy to each of the reads in our microbiome (where possible). This script involves multiple steps in the annotation process including:\n",
    "\n",
    "1. Running Kraken2 to assign reads to organisms by taxonomic rank. \n",
    "2. Running Bracken a tool that refines the Kraken2 output to try to reassign reads to higher species-level taxonomy\n",
    "3. Separating out the reads based on their taxonomy using KrakenTools. We do this at a high level human / non-human.\n",
    "\n",
    "The final part in this analysis looks for any remaining human contamination in the files, after we align to the human genome using bowtie2. Do you see any reads reported as human? Try \"Blasting\" these to see if they are indeed human reads that we missed by the alignment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to run Kraken, Bracken, and KrakenTools to assign taxonomy at the read-level.\n",
    "# A few important points:\n",
    "# 1. We are using the variables from the config file via the `source ./config.sh` command in the script.\n",
    "# 2. Kraken2 runs on each of the fastq files in the trimmed/human screened $FASTQ_DIR\n",
    "# 3. The results will be written into our $WORK_DIR\n",
    "# 4. Notice that we are asking for alot more resource (24 cores and 5G of memory per core), we are also asking for 10 hours of runtime.\n",
    "# But, runtime will likely be much less.\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bhurwitz\n",
    "#SBATCH --array=0-4                         \n",
    "#SBATCH --output=Job-read-taxonomy-%a.out\n",
    "#SBATCH --cpus-per-task=24\n",
    "#SBATCH --mem-per-cpu=5G  \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source $SLURM_SUBMIT_DIR/config.sh\n",
    "names=($(cat $XFILE_DIR/$XFILE))\n",
    "\n",
    "SAMPLE_ID=${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "### reads with human removed to match to the reference database\n",
    "PAIR1=${FASTQ_DIR}/${SAMPLE_ID}_1.fastq.gz\n",
    "PAIR2=${FASTQ_DIR}/${SAMPLE_ID}_2.fastq.gz\n",
    "\n",
    "KRAKEN_OUTDIR=${WORK_DIR}/out_reads_taxonomy\n",
    "OUTDIR=${KRAKEN_OUTDIR}/${SAMPLE_ID}\n",
    "HUMAN_READ_DIR=${OUTDIR}/human_reads\n",
    "NONHUMAN_READ_DIR=${OUTDIR}/nonhuman_reads\n",
    "\n",
    "### create the outdir if it does not exist\n",
    "if [[ ! -d \"$KRAKEN_OUTDIR\" ]]; then\n",
    "  echo \"$KRAKEN_OUTDIR does not exist. Directory created\"\n",
    "  mkdir $KRAKEN_OUTDIR\n",
    "fi\n",
    "\n",
    "if [[ ! -d \"$OUTDIR\" ]]; then\n",
    "  echo \"$OUTDIR does not exist. Directory created\"\n",
    "  mkdir $OUTDIR\n",
    "fi\n",
    "\n",
    "if [[ ! -d \"$HUMAN_READ_DIR\" ]]; then\n",
    "  echo \"$HUMAN_READ_DIR does not exist. Directory created\"\n",
    "  mkdir $HUMAN_READ_DIR\n",
    "fi\n",
    "\n",
    "if [[ ! -d \"$NONHUMAN_READ_DIR\" ]]; then\n",
    "  echo \"$NONHUMAN_READ_DIR does not exist. Directory created\"\n",
    "  mkdir $NONHUMAN_READ_DIR\n",
    "fi\n",
    "\n",
    "# check input\n",
    "echo ${PAIR1}\n",
    "echo ${PAIR2}\n",
    "echo ${OUTDIR}\n",
    "\n",
    "apptainer run ${KRAKEN2} kraken2 --db ${DB_DIR} --paired \\\n",
    "  --classified-out ${OUTDIR}/cseqs#.fq --output ${OUTDIR}/kraken_results.txt \\\n",
    "  --report ${OUTDIR}/kraken_report.txt --use-names --threads ${SLURM_CPUS_PER_TASK} \\\n",
    "  ${PAIR1} ${PAIR2}\n",
    "\n",
    "# refine hits with Bracken\n",
    "REPORT=\"${OUTDIR}/kraken_report.txt\"\n",
    "RESULTS=\"${OUTDIR}/kraken_results.txt\"\n",
    "apptainer run ${BRACKEN} est_abundance.py -i ${REPORT} -o ${OUTDIR}/bracken_results.txt -k ${DB_DIR}/database${KMER_SIZE}mers.kmer_distrib\n",
    "\n",
    "# get human and non-human reads (microbial)\n",
    "TAXID=9606\n",
    "HUMAN_R1=\"${HUMAN_READ_DIR}/r1.fq\"\n",
    "HUMAN_R2=\"${HUMAN_READ_DIR}/r2.fq\"\n",
    "\n",
    "BRACKEN_REPORT=\"${OUTDIR}/kraken_report_bracken_species.txt\"\n",
    "BRACKEN_RESULTS=\"${OUTDIR}/bracken_results.txt\"\n",
    "\n",
    "apptainer run ${KRAKENTOOLS} extract_kraken_reads.py -k ${RESULTS} \\\n",
    " -r ${BRACKEN_REPORT} -s1 ${PAIR1} -s2 ${PAIR2} --taxid ${TAXID} \\\n",
    " -o ${HUMAN_R1} -o2 ${HUMAN_R2} --include-children --fastq-output \n",
    "\n",
    "gzip ${HUMAN_READ_DIR}/r1.fq\n",
    "gzip ${HUMAN_READ_DIR}/r2.fq\n",
    "\n",
    "### selects all reads NOT from a given set of Kraken taxids (and all children)\n",
    "\n",
    "NONHUMAN_R1=\"${NONHUMAN_READ_DIR}/r1.fq\"\n",
    "NONHUMAN_R2=\"${NONHUMAN_READ_DIR}/r2.fq\"\n",
    "\n",
    "apptainer run ${KRAKENTOOLS} extract_kraken_reads.py -k ${RESULTS} \\\n",
    " -r ${BRACKEN_REPORT} -s1 ${PAIR1} -s2 ${PAIR2} --taxid ${TAXID} \\\n",
    " -o ${NONHUMAN_R1} -o2 ${NONHUMAN_R2} --include-children \\\n",
    " --exclude --fastq-output \n",
    "\n",
    "gzip ${NONHUMAN_READ_DIR}/r1.fq\n",
    "gzip ${NONHUMAN_READ_DIR}/r2.fq\n",
    "\n",
    "echo \"Finished `date`\"\n",
    "\n",
    "'''\n",
    "\n",
    "with open('run_read_taxonomy_parallel.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b9374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the code and make sure your script above was created.\n",
    "!cat run_read_taxonomy_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e42306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should be in your working directory when you run this script\n",
    "# do you see your config.sh file, and the run_kraken_parallel.sh script?\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe884278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run sbatch to run kraken2, bracken, and krakentools to assign taxonomy to our reads\n",
    "# Remember that this may take a while to run (~2 hours), so take a break, and get a coffee.\n",
    "!sbatch ./run_read_taxonomy_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome back\n",
    "# You can check if it is running using the squeue command\n",
    "# Check for all jobs under your netid\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e70ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check to see if there are any errors by looking at one of the job output files\n",
    "# Remember that this is going to give you a ton of data! These scripts report a lot in the log.\n",
    "# You can look at Job-rem_human-0.out\n",
    "!ls\n",
    "!tail Job-read-taxonomy-0.out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8b30b51",
   "metadata": {},
   "source": [
    "Nice, you should now have all of the reads assigned to different organisms. We are going to use this in our next steps to produce graphs and examine the taxonomic content of our samples.\n",
    "\n",
    "Let's take a quick peak to see if we were able to remove any additional human in the samples. Remember that this was supposed to be gone! But, alignment algorithms aren't perfect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the r1.fq.gz and r2.fq.gz files in your sample directories greater than zero in size?\n",
    "# Mine are, so likely you have some human contamination remaining.\n",
    "!ls -l $work_dir/out_reads_taxonomy/ERR*/human_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe3aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fear not, we removed all of the human contamination using krakentools to get all of the reads that don't match human\n",
    "# You can find them in the nonhuman_reads directory. \n",
    "!ls -l $work_dir/out_reads_taxonomy/ERR*/nonhuman_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5de26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also check some of these reads to see if they are actually human\n",
    "# Let's convert the first sequence in each of the files into fasta format\n",
    "!for dir in `ls ./out_reads_taxonomy`; do gzip -dc ./out_reads_taxonomy/$dir/human_reads/r1.fq.gz | head -2 | sed 's/^@/>/'; done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "362bc920",
   "metadata": {},
   "source": [
    "Now, copy and paste the 5 sequences from above and use the online BLAST tool to see if they are human.  To do this, \n",
    "\n",
    "1. Go to: https://blast.ncbi.nlm.nih.gov/Blast.cgi. \n",
    "2. Click on the Nucleotide BLAST button\n",
    "3. Paste your 5 fasta formatted sequences into the \"Enter Query Sequence\" box.\n",
    "4. Click on the \"BLAST\" button on the bottom of the page (which will use all defaults)\n",
    "\n",
    "What do you get? I found that most of my sequences had \"no significant match\", and one actually matched a bacterial genome. So, these reads may be less significant matches to the human genome (remember that we are using k-mers to find them and not entire sequences). They may also be matching to regions of the human genome that are known to be microbial in origin:\n",
    "\n",
    "[Microbial Genes in the Human Genome: Lateral Transfer or Gene Loss?](https://www.science.org/doi/10.1126/science.1061036)\n",
    "\n",
    "Or, oppositely, we can find human sequences in bacterial genomes:\n",
    "\n",
    "[Human contamination in bacterial genomes has created thousands of spurious proteins](https://genome.cshlp.org/content/29/6/954)\n",
    "\n",
    "Ugh, just when we thought we were getting somewhere!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dfb0bf5",
   "metadata": {},
   "source": [
    "## Step 2: Assigning taxonomy to the contigs from Megahit\n",
    "\n",
    "In this step, we will assign taxonomy to each of the contigs from our megahit assembly (where possible). This script involves multiple steps in the annotation process (just as before for the reads) including:\n",
    "\n",
    "1. Running Kraken2 to assign contigs to organisms by taxonomic rank. \n",
    "2. Running Bracken a tool that refines the Kraken2 output to try to reassign contigs to higher species-level taxonomy\n",
    "3. Separating out the contigs based on their taxonomy using KrakenTools. We do this at a high level human / non-human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67182c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to run Kraken, Bracken, and KrakenTools to assign taxonomy at the contig-level for megahit.\n",
    "# A few important points:\n",
    "# 1. We are using the variables from the config file via the `source ./config.sh` command in the script.\n",
    "# 2. Kraken2 runs on the final.contigs.fa file in the contig directory\n",
    "# 3. The results will be written into our $WORK_DIR\n",
    "# 4. Notice that we are asking for alot more resource (24 cores and 5G of memory per core), we are also asking for 10 hours of runtime.\n",
    "# But, runtime will likely be much less.\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bhurwitz\n",
    "#SBATCH --array=0-4                         \n",
    "#SBATCH --output=Job-megahit-taxonomy-%a.out\n",
    "#SBATCH --cpus-per-task=24\n",
    "#SBATCH --mem-per-cpu=5G  \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source $SLURM_SUBMIT_DIR/config.sh\n",
    "names=($(cat $XFILE_DIR/$XFILE))\n",
    "\n",
    "SAMPLE_ID=${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "### contigs\n",
    "CONTIGS=${MEGAHIT_CONTIG_DIR}/${SAMPLE_ID}/final.contigs.fa\n",
    "\n",
    "KRAKEN_OUTDIR=${WORK_DIR}/out_megahit_taxonomy\n",
    "OUTDIR=${KRAKEN_OUTDIR}/${SAMPLE_ID}\n",
    "HUMAN_READ_DIR=${OUTDIR}/human_contigs\n",
    "NONHUMAN_READ_DIR=${OUTDIR}/nonhuman_contigs\n",
    "\n",
    "### create the outdir if it does not exist\n",
    "if [[ ! -d \"$KRAKEN_OUTDIR\" ]]; then\n",
    "  echo \"$KRAKEN_OUTDIR does not exist. Directory created\"\n",
    "  mkdir $KRAKEN_OUTDIR\n",
    "fi\n",
    "                                  \n",
    "if [[ ! -d \"$OUTDIR\" ]]; then\n",
    "  echo \"$OUTDIR does not exist. Directory created\"\n",
    "  mkdir $OUTDIR\n",
    "fi\n",
    "\n",
    "if [[ ! -d \"$HUMAN_READ_DIR\" ]]; then\n",
    "  echo \"$HUMAN_READ_DIR does not exist. Directory created\"\n",
    "  mkdir $HUMAN_READ_DIR\n",
    "fi\n",
    "\n",
    "if [[ ! -d \"$NONHUMAN_READ_DIR\" ]]; then\n",
    "  echo \"$NONHUMAN_READ_DIR does not exist. Directory created\"\n",
    "  mkdir $NONHUMAN_READ_DIR\n",
    "fi\n",
    "\n",
    "apptainer run ${KRAKEN2} kraken2 --db ${DB_DIR} --classified-out \\\n",
    " ${OUTDIR}/cseqs#.fa --output ${OUTDIR}/kraken_results.txt \\\n",
    " --report ${OUTDIR}/kraken_report.txt --use-names --threads ${SLURM_CPUS_PER_TASK} \\\n",
    " ${CONTIGS}\n",
    "\n",
    "# refine hits with Bracken\n",
    "REPORT=\"${OUTDIR}/kraken_report.txt\"\n",
    "RESULTS=\"${OUTDIR}/kraken_results.txt\"\n",
    "apptainer run ${BRACKEN} est_abundance.py -i ${REPORT} -o ${OUTDIR}/bracken_results.txt -k ${DB_DIR}/database${KMER_SIZE}mers.kmer_distrib\n",
    "\n",
    "# get human and non-human reads (microbial)\n",
    "TAXID=9606\n",
    "HUMAN_R1=\"${HUMAN_READ_DIR}/contigs.fa\"\n",
    "\n",
    "BRACKEN_REPORT=\"${OUTDIR}/kraken_report_bracken_species.txt\"\n",
    "BRACKEN_RESULTS=\"${OUTDIR}/bracken_results.txt\"\n",
    "\n",
    "apptainer run ${KRAKENTOOLS} extract_kraken_reads.py -k ${RESULTS} \\\n",
    " -r ${BRACKEN_REPORT} -s1 ${CONTIGS} --taxid ${TAXID} -o ${HUMAN_R1} \\\n",
    " --include-children \n",
    "\n",
    "if [[ -f \"${HUMAN_READ_DIR}/contigs.fa\" ]]; then\n",
    "    gzip ${HUMAN_READ_DIR}/contigs.fa\n",
    "fi\n",
    "\n",
    "### selects all reads NOT from a given set of Kraken taxids (and all children)\n",
    "\n",
    "NONHUMAN_R1=\"${NONHUMAN_READ_DIR}/contigs.fa\"\n",
    "\n",
    "apptainer run ${KRAKENTOOLS} extract_kraken_reads.py -k ${RESULTS} \\\n",
    " -r ${BRACKEN_REPORT} -s1 ${CONTIGS} --taxid ${TAXID} -o ${NONHUMAN_R1} \\\n",
    " --include-children --exclude \n",
    "\n",
    "if [[ -f \"${NONHUMAN_READ_DIR}/contigs.fa\" ]]; then\n",
    "    gzip ${NONHUMAN_READ_DIR}/contigs.fa\n",
    "fi\n",
    "\n",
    "echo \"Finished `date`\"\n",
    "\n",
    "'''\n",
    "\n",
    "with open('run_megahit_taxonomy_parallel.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a07b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's kick this off!\n",
    "!sbatch ./run_megahit_taxonomy_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba643a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check back later to see if you were able to get human contigs. Are the file sizes > 0?\n",
    "!ls -l $work_dir/out_megahit_taxonomy/ERR*/human_contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb76e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you see contigs assigned to non-human organisms? (aka microbes)\n",
    "!ls -l $work_dir/out_megahit_taxonomy/ERR*/nonhuman_contigs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af22da97",
   "metadata": {},
   "source": [
    "## Step 3: Assigning taxonomy to the contigs from Metaspades\n",
    "\n",
    "In this step, we will assign taxonomy to each of the contigs from our metaspades assembly (where possible). This script involves multiple steps in the annotation process (just as before for the reads) including:\n",
    "\n",
    "1. Running Kraken2 to assign contigs to organisms by taxonomic rank. \n",
    "2. Running Bracken a tool that refines the Kraken2 output to try to reassign contigs to higher species-level taxonomy\n",
    "3. Separating out the contigs based on their taxonomy using KrakenTools. We do this at a high level human / non-human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to run Kraken, Bracken, and KrakenTools to assign taxonomy at the contig-level for metaspades.\n",
    "# A few important points:\n",
    "# 1. We are using the variables from the config file via the `source ./config.sh` command in the script.\n",
    "# 2. Kraken2 runs on the final.contigs.fa file in the contig directory\n",
    "# 3. The results will be written into our $WORK_DIR\n",
    "# 4. Notice that we are asking for alot more resource (24 cores and 5G of memory per core), we are also asking for 10 hours of runtime.\n",
    "# But, runtime will likely be much less.\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bhurwitz\n",
    "#SBATCH --array=0-4                         \n",
    "#SBATCH --output=Job-metaspades-taxonomy-%a.out\n",
    "#SBATCH --cpus-per-task=24\n",
    "#SBATCH --mem-per-cpu=5G  \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source $SLURM_SUBMIT_DIR/config.sh\n",
    "names=($(cat $XFILE_DIR/$XFILE))\n",
    "\n",
    "SAMPLE_ID=${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "### contigs\n",
    "CONTIGS=${METASPADES_CONTIG_DIR}/${SAMPLE_ID}/contigs.fasta\n",
    "\n",
    "KRAKEN_OUTDIR=${WORK_DIR}/out_metaspades_taxonomy\n",
    "OUTDIR=${KRAKEN_OUTDIR}/${SAMPLE_ID}\n",
    "HUMAN_READ_DIR=${OUTDIR}/human_contigs\n",
    "NONHUMAN_READ_DIR=${OUTDIR}/nonhuman_contigs\n",
    "\n",
    "### create the outdir if it does not exist\n",
    "if [[ ! -d \"$KRAKEN_OUTDIR\" ]]; then\n",
    "  echo \"$KRAKEN_OUTDIR does not exist. Directory created\"\n",
    "  mkdir $KRAKEN_OUTDIR\n",
    "fi\n",
    "                                  \n",
    "if [[ ! -d \"$OUTDIR\" ]]; then\n",
    "  echo \"$OUTDIR does not exist. Directory created\"\n",
    "  mkdir $OUTDIR\n",
    "fi\n",
    "\n",
    "if [[ ! -d \"$HUMAN_READ_DIR\" ]]; then\n",
    "  echo \"$HUMAN_READ_DIR does not exist. Directory created\"\n",
    "  mkdir $HUMAN_READ_DIR\n",
    "fi\n",
    "\n",
    "if [[ ! -d \"$NONHUMAN_READ_DIR\" ]]; then\n",
    "  echo \"$NONHUMAN_READ_DIR does not exist. Directory created\"\n",
    "  mkdir $NONHUMAN_READ_DIR\n",
    "fi\n",
    "\n",
    "apptainer run ${KRAKEN2} kraken2 --db ${DB_DIR} --classified-out \\\n",
    " ${OUTDIR}/cseqs#.fa --output ${OUTDIR}/kraken_results.txt \\\n",
    " --report ${OUTDIR}/kraken_report.txt --use-names --threads ${SLURM_CPUS_PER_TASK} \\\n",
    " ${CONTIGS}\n",
    "\n",
    "# refine hits with Bracken\n",
    "REPORT=\"${OUTDIR}/kraken_report.txt\"\n",
    "RESULTS=\"${OUTDIR}/kraken_results.txt\"\n",
    "apptainer run ${BRACKEN} est_abundance.py -i ${REPORT} -o ${OUTDIR}/bracken_results.txt -k ${DB_DIR}/database${KMER_SIZE}mers.kmer_distrib\n",
    "\n",
    "# get human and non-human reads (microbial)\n",
    "TAXID=9606\n",
    "HUMAN_R1=\"${HUMAN_READ_DIR}/contigs.fa\"\n",
    "\n",
    "BRACKEN_REPORT=\"${OUTDIR}/kraken_report_bracken_species.txt\"\n",
    "BRACKEN_RESULTS=\"${OUTDIR}/bracken_results.txt\"\n",
    "\n",
    "apptainer run ${KRAKENTOOLS} extract_kraken_reads.py -k ${RESULTS} \\\n",
    " -r ${BRACKEN_REPORT} -s1 ${CONTIGS} --taxid ${TAXID} -o ${HUMAN_R1} \\\n",
    " --include-children\n",
    "\n",
    "if [[ -f \"${HUMAN_READ_DIR}/contigs.fa\" ]]; then\n",
    "    gzip ${HUMAN_READ_DIR}/contigs.fa\n",
    "fi\n",
    "\n",
    "### selects all reads NOT from a given set of Kraken taxids (and all children)\n",
    "\n",
    "NONHUMAN_R1=\"${NONHUMAN_READ_DIR}/contigs.fa\"\n",
    "\n",
    "apptainer run ${KRAKENTOOLS} extract_kraken_reads.py -k ${RESULTS} \\\n",
    " -r ${BRACKEN_REPORT} -s1 ${CONTIGS} --taxid ${TAXID} -o ${NONHUMAN_R1} \\\n",
    " --include-children --exclude\n",
    "\n",
    "if [[ -f \"${NONHUMAN_READ_DIR}/contigs.fa\" ]]; then\n",
    "    gzip ${NONHUMAN_READ_DIR}/contigs.fa\n",
    "fi\n",
    "\n",
    "echo \"Finished `date`\"\n",
    "\n",
    "'''\n",
    "\n",
    "with open('run_metaspades_taxonomy_parallel.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kick off the script\n",
    "!sbatch ./run_metaspades_taxonomy_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check back later to see if you were able to get human contigs. Are the file sizes > 0?\n",
    "!ls -l $work_dir/out_metaspades_taxonomy/ERR*/human_contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about microbial contigs?\n",
    "!ls -l $work_dir/out_metaspades_taxonomy/ERR*/nonhuman_contigs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a1df838",
   "metadata": {},
   "source": [
    "Great job! We will compare and contrast the taxonomic annotation for the assemblies and the reads in the next homework assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83501ab5",
   "metadata": {},
   "source": [
    "## Final Step\n",
    "Copy your notebook to the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp ~/11_taxonomy.ipynb $work_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
