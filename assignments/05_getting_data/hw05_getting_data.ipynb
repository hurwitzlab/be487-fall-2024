{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW05 getting data\n",
    "\n",
    "### Downloading FASTQ files from the SRA\n",
    "\n",
    "Now comes the exciting part. We are going to get started on your project. The first step in this process is to download data from the Sequence Read Archive or SRA. This notebook will walk you through all of the steps in this process using a tool called the sra-toolkit. \n",
    "\n",
    "-----------\n",
    "\n",
    "Sections:\n",
    "\n",
    "1. Creating a run script to prefetch your FASTQ files\n",
    "2. Creating a run script to use fasterq-dump to download FASTQ R1 and R2 (forward and reverse reads)\n",
    "3. Creating a run script to zip up your FASTQ files\n",
    "4. Running a launcher script to submit your run scripts to the cluster to run\n",
    "\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Before we get started you will need to set several variables that we will use throughout this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the variables for your netid\n",
    "netid = \"MY_NETID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and change into this directory\n",
    "# All files will be downoaded here. All scripts will be written here.\n",
    "work_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/assignments/05_getting_data\"\n",
    "%cd $work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the fastq directory\n",
    "# Notice this is the same as the working directory, because we are downloading the fastq files \n",
    "# for the first time. We will use this same directory in other Jupyter notebooks for the project.\n",
    "fastq_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/assignments/05_getting_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will need to see what \"xfile\" you have been assigned to.\n",
    "# List the directory to find the name of the file that starts with \"x\"\n",
    "# You will use this file for every homework assignment.\n",
    "!ls x*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of your xfile\n",
    "# replace \"MY_XFILE\" with the one above\n",
    "xfile = \"MY_XFILE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a config file\n",
    "The scripts below executes code that requires certain variables to be set. So we don't need to edit the code in the script, we are going to use a config file that defines all of these variables for us. Then when we want to use these variables in the script, we will use a command called \"source\" to set the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config file with all of the variables you need\n",
    "!echo \"export NETID=$netid\" > config.sh\n",
    "!echo \"export XFILE=$xfile\" >> config.sh\n",
    "!echo \"export SRA_TOOLKIT=/contrib/singularity/shared/bhurwitz/sra-tools-3.0.3.sif\" >> config.sh\n",
    "!echo \"export WORK_DIR=$work_dir\" >> config.sh\n",
    "!echo \"export FASTQ_DIR=$fastq_dir\" >> config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the config file to be sure it is correct\n",
    "# Is your netid and xfile correct? Do you have the right directories?\n",
    "!cat config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Writing a run script to prefetch the FASTQ files for your project\n",
    "\n",
    "The very first step in downloading data from the Sequence Read Archive (SRA) at NCBI is to \"pre-fetch\" the data using the SRA toolkit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using containers to run bioinformatics tools\n",
    "\n",
    "We will be running many bioinformatics tools using containers. Containers are virtual machines that contain all of the necessary components to run the code. This includes the operating system, the bioinformatics tool, and any dependencies. Containers allow programmers to \"package\" up their code, so it can be run anywhere (on a laptop, HPC, or in the cloud) without having to reinstall and set everything up to run the code there locally. Everything is in the container!\n",
    "\n",
    "The UA HPC requires us to use the apptainer command to create/run our bioinformatics tools in containers. The command to run a container looks something like this:\n",
    "\n",
    "apptainer run NAME_OF_TOOL command [options and files]\n",
    "\n",
    "Here is an example for the SRA Toolkit that we will be using here:\n",
    "\n",
    "```\n",
    "apptainer run ${SRA_TOOLKIT} prefetch [options and files]\n",
    "\n",
    "```\n",
    "\n",
    "The apptainer command can only be run from one of the compute nodes, not the login node. This means that we need to put this code inside a shell script to run it. I called this a \"run script\". We then use the sbatch command to \"launch\" this script on the HPC. I call this script the \"launcher script\".\n",
    "\n",
    "OK, let's get started by creating our \"run_scripts\". These scripts will run our containers (or bioinformatics code).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the run script to pre-fetch FASTQ files by using Python to write it for us.\n",
    "\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-7                       \n",
    "#SBATCH --output=05A_run_prefetch-%a.out\n",
    "#SBATCH --error=05A_run_prefetch-%a.err\n",
    "#SBATCH --cpus-per-task=4                    \n",
    "#SBATCH --mem-per-cpu=2G                            \n",
    " \n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat ${FASTQ_DIR}/${XFILE}))\n",
    " \n",
    "echo ${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "apptainer run ${SRA_TOOLKIT} prefetch ${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "'''\n",
    "\n",
    "with open('05A_run_prefetch.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Writing a run script to download FASTQ files for your project\n",
    "\n",
    "After we prefetch all of the FASTQ files, we need to download them. We will use the fasterq-dump command to get the FASTQ R1 and R2 files. \n",
    "\n",
    "Here's the fasterq-dump [documentation](https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the run script to get the FASTQ files by using Python to write it for us.\n",
    "\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-7                        \n",
    "#SBATCH --output=05B_run_fasterq-dump-%a.out\n",
    "#SBATCH --error=05B_run_fasterq-dump-%a.err\n",
    "#SBATCH --cpus-per-task=4                    \n",
    "#SBATCH --mem-per-cpu=2G                            \n",
    " \n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat ${FASTQ_DIR}/${XFILE}))\n",
    " \n",
    "echo ${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "apptainer run ${SRA_TOOLKIT} fasterq-dump --split-files ${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "'''\n",
    "\n",
    "with open('05B_run_fasterq-dump.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Writing a run script to compress the FASTQ files for your project\n",
    "\n",
    "The FASTQ files for your project are huge. To stay in the good graces of our HPC staff and keep our file sizes down so we don't run out of space, we will compress our FASTQ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a run script that gzip's all of the FASTQ files\n",
    "# These are huge files, so it may take some time to run.\n",
    "# This script uses gzip to compress each of the *.fastq files in your fastq_dir.\n",
    "\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-7\n",
    "#SBATCH --output=05B_run_gzip-%a.out\n",
    "#SBATCH --cpus-per-task=2   \n",
    "#SBATCH --mem-per-cpu=6G              \n",
    " \n",
    "pwd; hostname; date\n",
    "source ./config.sh\n",
    "names=($(cat ${FASTQ_DIR}/${XFILE}))\n",
    "\n",
    "gzip ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq\n",
    "\n",
    "'''\n",
    "\n",
    "with open('05C_run_gzip.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Putting it all together\n",
    "\n",
    "Once you have created the run scripts, you are ready to put them together in a pipeline to run each one, one after another. Each run scipt will be a \"job\" and each job will wait for the one before it to finish before starting.\n",
    "\n",
    "For example, the 05A_run_prefetch job need to finish, before we can run the 05B_run_fasterq-dump job. To do this, we will need to set up dependencies in our \"launch script\". Also, notice that each job is a job array, meaning that it is comprised of multiple jobs within it. In our case each job array has 8 elements in it, one for each accession we are running through that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the launcher script to kick off our pipeline.\n",
    "\n",
    "my_code = '''#! /bin/bash\n",
    "\n",
    "# 05A_run_prefetch: first job - no dependencies\n",
    "job1=$(sbatch 05A_run_prefetch.sh)\n",
    "jid1=$(echo $job1 | sed 's/^Submitted batch job //')\n",
    "echo $jid1\n",
    "\n",
    "# 05B_run_fasterq-dump: jid2 depends on jid1\n",
    "job2=$(sbatch --dependency=afterok:$jid1 05B_run_fasterq-dump.sh)\n",
    "jid2=$(echo $job2 | sed 's/^Submitted batch job //')\n",
    "echo $jid2\n",
    "\n",
    "# 05C_run_gzip: jid3 depends on jid2\n",
    "job3=$(sbatch --dependency=afterok:$jid2 05C_run_gzip.sh)\n",
    "jid3=$(echo $job3 | sed 's/^Submitted batch job //')\n",
    "echo $jid3\n",
    "\n",
    "'''\n",
    "\n",
    "with open('05_launch_pipeline.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the pipeline script executable\n",
    "!chmod +x *.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's run it!\n",
    "!./05_launch_pipeline.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if it is running using the squeue command\n",
    "# Check for all jobs under your netid\n",
    "# Notice that 05B jobs are dependent on 05A jobs finishing.\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens next?\n",
    "\n",
    "Your code will take a little time to get \"picked up\" by the HPC and move from PD (pending) to R (running). Come back in about a day to double check you got all of the raw sequence files using the hw05_check.ipynb notebook. But, for now, relax and enjoy your day!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The End! \n",
    "Be sure to copy your notebook into the class project directory (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ~/be487-fall-2024/assignments/05_getting_data/hw05_getting_data.ipynb $work_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "df955ce39d0f31d56d4bb2fe0a613e5326ba60723fd33d8303a3aede8f65715c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
