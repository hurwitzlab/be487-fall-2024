{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de7bac8",
   "metadata": {},
   "source": [
    "# Metagenomic Binning & quality\n",
    "\n",
    "This notebook will go through the workflow for binning contigs into species-level bins from a metagenome assembled genome (MAG). Once the contigs are binned, we will assess the quality and completeness of the genomes in the bins.\n",
    "\n",
    "-----------\n",
    "\n",
    "Sections:\n",
    "\n",
    "1. Create species-level bins for your metagenome assembled genomes (MAGs).\n",
    "2. Rename your bins for further processing\n",
    "3. Use Quast to get stats on the MAGs.\n",
    "4. Use CheckM to assess the quality of your species-level bins and MAGs.\n",
    "5. Launch the pipeline to run each of the run scripts.\n",
    "\n",
    "-----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebed36",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "You will need to rerun this section each time you come back to this notebook to reset all directories and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a46521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the variables for your netid and xfile\n",
    "netid = \"YOUR_NETID\"\n",
    "xfile = \"YOUR_XFILE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d388ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into the working directory\n",
    "work_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/assignments/09_binning_quality\"\n",
    "xfile_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/assignments/05_getting_data\"\n",
    "fastq_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/assignments/07_contam_removal\"\n",
    "%cd $work_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380dffed",
   "metadata": {},
   "source": [
    "## Creating a config file\n",
    "The scripts below executes code that requires certain variables to be set. So we don't need to edit the code in the script, we are going to use a config file that defines all of these variables for us. Then when we want to use these variables in the script, we will \"source\" the config file to set the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cf341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config file with all of the variables you need\n",
    "# notice that we are using the reads post-trimming, and post-human removal\n",
    "!echo \"export NETID=$netid\" > config.sh\n",
    "!echo \"export XFILE=$xfile\" >> config.sh\n",
    "!echo \"export WORK_DIR=$work_dir\" >> config.sh\n",
    "!echo \"export XFILE_DIR=$xfile_dir\" >> config.sh\n",
    "!echo \"export FASTQ_DIR=$fastq_dir\" >> config.sh\n",
    "!echo \"export MEGAHIT_DIR=/xdisk/bhurwitz/bh_class/$netid/assignments/08_assembly/out_megahit\" >> config.sh\n",
    "!echo \"export BWA=/contrib/singularity/shared/bhurwitz/bwa:0.7.8--he4a0461_9.sif\" >> config.sh\n",
    "!echo \"export SAMTOOLS=/contrib/singularity/shared/bhurwitz/samtools:1.17--hd87286a_1.sif\" >> config.sh\n",
    "!echo \"export CONCOCT=/contrib/singularity/shared/bhurwitz/concoct:1.1.0--py311h245ed52_4.sif\" >> config.sh\n",
    "!echo \"export QUAST=/contrib/singularity/shared/bhurwitz/quast:5.2.0--py39pl5321h4e691d4_3.sif\" >> config.sh\n",
    "!echo \"export CHECKM=/contrib/singularity/shared/bhurwitz/checkm2\\:1.0.1--pyh7cba7a3_0.sif\" >> config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the config file to be sure it is correct\n",
    "# Is your netid and xfile correct? Do you have the right directories?\n",
    "!cat config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeaabfe",
   "metadata": {},
   "source": [
    "## Step 1: 09A_binning\n",
    "\n",
    "Aligning reads to your megahit contigs via bwa, and binning with concoct \n",
    "\n",
    "In this step, we will align the reads from your \"screened and cleaned\" fastq file back to the contigs you created using each of the assemblers: metaspades and megahit. In the next step, we will use this information to determine the \"coverage\" for each of the contigs. These data will be used by the binning step to place contigs into the same species-level (hopefully single organism) bins based on the coverage and sequence composition of the contigs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to align your reads to each of your assemblies, and then bin\n",
    "# A few important points:\n",
    "# 1. We are using the variables from the config file via the `source ./config.sh` command in the script.\n",
    "# 2. bwa aligns each of the fastq files in the trimmed and human filtered $FASTQ_DIR\n",
    "# 3. We then run concoct to bin the contigs\n",
    "# The results will be written into our $WORK_DIR\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-7                         \n",
    "#SBATCH --output=09A_binning-%a.out\n",
    "#SBATCH --cpus-per-task=24\n",
    "#SBATCH --mem=8G                                  \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat $XFILE_DIR/$XFILE))\n",
    "\n",
    "SAMPLE_ID=${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "### reads after trimming and human filtering\n",
    "PAIR1=${FASTQ_DIR}/${SAMPLE_ID}_1.fastq.gz\n",
    "PAIR2=${FASTQ_DIR}/${SAMPLE_ID}_2.fastq.gz\n",
    "\n",
    "CONCOCT_OUTDIR=${WORK_DIR}/out_concoct\n",
    "OUTDIR=${CONCOCT_OUTDIR}/${SAMPLE_ID}\n",
    "\n",
    "### create the outdir if it does not exist\n",
    "if [[ ! -d \"$CONCOCT_OUTDIR\" ]]; then\n",
    "  echo \"$CONCOCT_OUTDIR does not exist. Directory created\"\n",
    "  mkdir $CONCOCT_OUTDIR\n",
    "fi\n",
    "\n",
    "if [[ ! -d \"$OUTDIR\" ]]; then\n",
    "  echo \"$OUTDIR does not exist. Directory created\"\n",
    "  mkdir $OUTDIR\n",
    "fi\n",
    "\n",
    "### final contigs (our input file for binning)\n",
    "CONTIGS=\"${MEGAHIT_DIR}/${SAMPLE_ID}/final.contigs.fa\"\n",
    "\n",
    "### create the index from the contigs\n",
    "apptainer run ${BWA} bwa index ${CONTIGS}\n",
    "\n",
    "### align reads to the index\n",
    "apptainer run ${BWA} bwa mem \\\n",
    "-t $SLURM_CPUS_PER_TASK \\\n",
    "${CONTIGS} \\\n",
    "${PAIR1} \\\n",
    "${PAIR2} \\\n",
    "> $OUTDIR/result.sam\n",
    "\n",
    "### convert sam to bam\n",
    "apptainer run ${SAMTOOLS} samtools view \\\n",
    "-b -F 4 ${OUTDIR}/result.sam > ${OUTDIR}/result.bam\n",
    "\n",
    "apptainer run ${SAMTOOLS} samtools \\\n",
    "sort ${OUTDIR}/result.bam > ${OUTDIR}/my_sorted.bam\n",
    "\n",
    "apptainer run ${SAMTOOLS} samtools \\\n",
    "index ${OUTDIR}/my_sorted.bam\n",
    "\n",
    "### run the binning step\n",
    "### following the protocol here: https://concoct.readthedocs.io/en/latest/usage.html\n",
    "echo \"Starting cut_up_fasta.py\"\n",
    "MIN_CONTIG_LENGTH=10000\n",
    "apptainer run ${CONCOCT} cut_up_fasta.py \\\n",
    "${CONTIGS} \\\n",
    "--chunk_size ${MIN_CONTIG_LENGTH} \\\n",
    "--overlap_size 0 \\\n",
    "--bedfile ${OUTDIR}/contigs_10k.bed \\\n",
    "--merge_last \\\n",
    "> ${OUTDIR}/contigs_10k.fa\n",
    "\n",
    "echo \"Starting concoct_coverage_table.py\"\n",
    "apptainer run ${CONCOCT} concoct_coverage_table.py \\\n",
    "${OUTDIR}/contigs_10k.bed ${OUTDIR}/my_sorted.bam > ${OUTDIR}/coverage_table.tsv\n",
    "\n",
    "echo \"Starting concoct\"\n",
    "apptainer run ${CONCOCT} concoct --threads 24 \\\n",
    "--composition_file ${OUTDIR}/contigs_10k.fa --coverage_file ${OUTDIR}/coverage_table.tsv -b ${OUTDIR}\n",
    "\n",
    "echo \"Starting merge_cutup_clustering.py\"\n",
    "apptainer run ${CONCOCT} \\\n",
    "merge_cutup_clustering.py ${OUTDIR}/clustering_gt1000.csv > ${OUTDIR}/clustering_merged.csv\n",
    "\n",
    "echo \"Starting extract_fasta_bins.py\"\n",
    "mkdir ${OUTDIR}/fasta_bins\n",
    "apptainer run ${CONCOCT} \\\n",
    "extract_fasta_bins.py ${CONTIGS} ${OUTDIR}/clustering_merged.csv --output_path ${OUTDIR}/fasta_bins\n",
    "\n",
    "'''\n",
    "\n",
    "with open('09A_binning.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8c919",
   "metadata": {},
   "source": [
    "## Step 2: 09B_add_bin_nums\n",
    "\n",
    "Let's rename the bins and combine into a single file\n",
    "\n",
    "Rock on! The last run script creates bins for your megahit contigs. These bins should represent the species (and individual organisms) present in your samples.\n",
    "\n",
    "The next run script will generate a series of files for each of your samples. In particular we will create a series of *.fasta files preceeded by numbers. These are the different genome bins predicted by binning. Then we will combine these into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to rename genome bins\n",
    "# A few important points:\n",
    "# 1. We are using the variables from the config file via the `source ./config.sh` command\n",
    "# 2. The results will be written into our $WORK_DIR\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=01:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class                        \n",
    "#SBATCH --output=09B_add_bin_nums-%a.out\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --mem-per-cpu=5G \n",
    "source ./config.sh\n",
    "names=($(cat $XFILE_DIR/$XFILE))\n",
    "\n",
    "CONCOCT_OUTDIR=${WORK_DIR}/out_concoct\n",
    "\n",
    "cd $CONCOCT_OUTDIR\n",
    "\n",
    "for i in {0..7}; do\n",
    "    SAMPLE_ID=${names[$i]}\n",
    "    BIN_DIR=${CONCOCT_OUTDIR}/${SAMPLE_ID}/fasta_bins\n",
    "    echo ${SAMPLE_ID}\n",
    "    touch ${SAMPLE_ID}.all_contigs.fna\n",
    "    cd ${BIN_DIR}\n",
    "    for file in *.fa; do\n",
    "        num=$(echo $file | sed 's/.fa//')\n",
    "        cat $num.fa | sed -e \"s/^>/>${num}_/\" >> $CONCOCT_OUTDIR/${SAMPLE_ID}.all_contigs.fna\n",
    "    done\n",
    "    cd $CONCOCT_OUTDIR\n",
    "done\n",
    "\n",
    "cd $WORK_DIR\n",
    "'''\n",
    "\n",
    "with open('09B_add_bin_nums.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ce3e5",
   "metadata": {},
   "source": [
    "## Step 3: 09C_quast\n",
    "\n",
    "Quast (checking the quality of our assembly)\n",
    "\n",
    "How good are our assemblies? We can check the quality by running tools that look at the contigs produced by our assembly algorithms. \n",
    "\n",
    "Let's see what the quality of our assemblies for megahit, using a bioinformatics tool called quast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to run Quast on each of our contig files\n",
    "# A few important points:\n",
    "# 1. We are using the variables from the config file via the `source ./config.sh` command\n",
    "# 2. Quast runs on the contigs files in the MEGAHIT_DIR and METASPADES_DIR\n",
    "# 3. The results will be written into our $WORK_DIR\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=12:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-7                         \n",
    "#SBATCH --output=09C_quast-%a.out\n",
    "#SBATCH --cpus-per-task=24\n",
    "#SBATCH --mem-per-cpu=5G                                    \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat $XFILE_DIR/$XFILE))\n",
    "\n",
    "SAMPLE_ID=${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "### create output directories for the reports\n",
    "### note that we are going to compare both assemblies at once\n",
    "OUTDIR=${WORK_DIR}/out_quast\n",
    "\n",
    "### create the outdir if it does not exist\n",
    "if [[ ! -d \"$OUTDIR\" ]]; then\n",
    "  echo \"$OUTDIR does not exist. Directory created\"\n",
    "  mkdir $OUTDIR\n",
    "fi\n",
    "\n",
    "### Contigs to use post-binning\n",
    "CONCOCT_OUTDIR=${WORK_DIR}/out_concoct\n",
    "CONCOCT_CONTIGS=$CONCOCT_OUTDIR/${SAMPLE_ID}.all_contigs.fna\n",
    "\n",
    "### Run Quast\n",
    "apptainer run ${QUAST} quast -t 24 \\\n",
    "        -o $OUTDIR/${SAMPLE_ID} \\\n",
    "        -m 500 \\\n",
    "        $CONCOCT_CONTIGS\n",
    "'''\n",
    "\n",
    "with open('09C_quast.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84066ebe",
   "metadata": {},
   "source": [
    "## Step 4: 09D_checkm\n",
    "\n",
    "Checkm2 is another tool that allows you to produce a quality report on the assembled contigs.\n",
    "\n",
    "The documentation can be found [here](https://github.com/chklovski/CheckM2).\n",
    "\n",
    "This tool requires a database file to run. More information on downloading the database can be found in the documentation. The current database has been downloaded and saved in the following location:\n",
    "\n",
    "/groups/bhurwitz/databases/checkm2_database/uniref100.KO.1.dmnd\n",
    "\n",
    "Let's create a run script to run checkM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1675ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to run on each of bins\n",
    "# A few important points:\n",
    "# 1. We are using the variables from the config file via the `source ./config.sh` command \n",
    "# 2. CheckM runs on the bin files in the MEGAHIT_DIR and METASPADES_DIR\n",
    "# 3. The results will be written into our $WORK_DIR\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=24:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-7                       \n",
    "#SBATCH --output=09D_checkm-%a.out\n",
    "#SBATCH --cpus-per-task=24\n",
    "#SBATCH --mem-per-cpu=5G                                    \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat $XFILE_DIR/$XFILE))\n",
    "\n",
    "SAMPLE_ID=${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "### create output directory for the report\n",
    "OUTDIR=${WORK_DIR}/out_checkm\n",
    "\n",
    "### create the outdirs if they do not exist\n",
    "if [[ ! -d \"$CHECKM_OUTDIR\" ]]; then\n",
    "  echo \"$CHECKM_OUTDIR does not exist. Directory created\"\n",
    "  mkdir $CHECKM_OUTDIR\n",
    "fi\n",
    "\n",
    "### Contigs to use post-binning\n",
    "CONCOCT_OUTDIR=${WORK_DIR}/out_concoct\n",
    "CONCOCT_CONTIGS=${CONCOCT_OUTDIR}/${SAMPLE_ID}/fasta_bins\n",
    "\n",
    "### Run Megahit\n",
    "apptainer run ${CHECKM} checkm2 \\\n",
    "        predict --threads 24 \\\n",
    "        --input $CONCOCT_CONTIGS \\\n",
    "        -x fa \\\n",
    "        --output-directory $OUTDIR/${SAMPLE_ID} \\\n",
    "        --database_path /groups/bhurwitz/databases/checkm2_database/uniref100.KO.1.dmnd  \n",
    "'''\n",
    "\n",
    "with open('09D_checkm.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b60d0",
   "metadata": {},
   "source": [
    "## Step 5: Putting it all together\n",
    "\n",
    "Once you have created the the run scripts, you are ready to put them together in a pipeline to run each of the steps one by one. Notice which steps are dependent on the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43453aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the launcher script to kick off our pipeline.\n",
    "\n",
    "my_code = '''#! /bin/bash\n",
    "\n",
    "# 09A_binning: first job - no dependencies\n",
    "job1=$(sbatch 09A_binning.sh)\n",
    "jid1=$(echo $job1 | sed 's/^Submitted batch job //')\n",
    "echo $jid1\n",
    "\n",
    "# 09B_add_bin_nums: jid2 depends on jid1\n",
    "job2=$(sbatch --dependency=afterok:$jid1 09B_add_bin_nums.sh)\n",
    "jid2=$(echo $job2 | sed 's/^Submitted batch job //')\n",
    "echo $jid2\n",
    "\n",
    "# 09C_quast: jid3 depends on jid2\n",
    "job3=$(sbatch --dependency=afterok:$jid2 09C_quast.sh)\n",
    "jid3=$(echo $job3 | sed 's/^Submitted batch job //')\n",
    "echo $jid3\n",
    "\n",
    "# 09D_checkm: jid4 depends on jid3\n",
    "job4=$(sbatch --dependency=afterok:$jid3 09D_checkm.sh)\n",
    "jid4=$(echo $job4 | sed 's/^Submitted batch job //')\n",
    "echo $jid4\n",
    "\n",
    "'''\n",
    "\n",
    "with open('09_launch_pipeline.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ad02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the pipeline script executable\n",
    "!chmod +x *.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's run it!\n",
    "!./09_launch_pipeline.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe2c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if it is running using the squeue command\n",
    "# Check for all jobs under your netid\n",
    "# Note that this will take some time to run, so go get a coffee!\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83501ab5",
   "metadata": {},
   "source": [
    "## Final Step\n",
    "Copy your notebook to the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ~/be487-fall-2024/assignments/09_binning_quality/hw09_binning_quality.ipynb $work_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
