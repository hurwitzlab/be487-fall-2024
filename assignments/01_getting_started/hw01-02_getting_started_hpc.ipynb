{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the UA Campus HPC\n",
    "\n",
    "### Questions:\n",
    "- What is a super computer?\n",
    "- How do I login to th HPC?\n",
    "- What is a Bastion host?\n",
    "- What is a login node?\n",
    "- What is a job scheduler?\n",
    "- What is a node?\n",
    "- Why do I need to use an HPC?\n",
    "- Where do I store my files?\n",
    "- How can I run jobs interactively?\n",
    "- What software can I run?\n",
    "- What are batch jobs?\n",
    "- Show me some examples...\n",
    "\n",
    "### Objectives:\n",
    "- Learn how to runs jobs on the campus HPC (you will be guided through ths via Jupter notebooks!)\n",
    "- Learn about the different computers that make up an HPC\n",
    "- Learn how to use high-performance computing techniques to run many jobs at once\n",
    "\n",
    "### Keypoints:\n",
    "\n",
    "- The campus HPC is an amazing resource! We are so lucky.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's a supercomputer?\n",
    "A supercomputer is a collection, or cluster, of a large number of regular computers (referred to as compute nodes) connected over a network. Each of the computers is like a local workstation though typically much more capable. For example, a standard laptop might have 4 CPUs and 8gb of RAM. Compare this with a standard compute node on Puma which has a whopping 94 CPUs and 470gb of RAM!\n",
    "\n",
    "<br>\n",
    "Another thing that differentiates supercomputers from your personal workstation is a supercomputer is a shared resource. This means there may be hundreds or even thousands of simultaneous users. Without some sort of coordination, you can imagine it would be a logistical nightmare to figure out who can run their code, what resources they can use, and where they should run it. That's why supercomputers use login nodes and job schedulers (we use one called SLURM).\n",
    "\n",
    "<br>\n",
    "\n",
    "<a href=\"../../fig/HPCDiagram_FileTransfers.png\">\n",
    "  <img src=\"../../fig/HPCDiagram_FileTransfers.png\" alt=\"Cluster Overview.\"/>\n",
    "</a>\n",
    "\n",
    "## Logging into the HPC.\n",
    "In past exercises, you have learned how to login to the HPC and have either used the [ondemand web portal](https://ood.hpc.arizona.edu/pun/sys/dashboard), or you have logged in directory via a shell program from your laptop. If you login from the HPC ondemand portal using the Clusters -> Shell from the menu, you will be directed to the Bastion host, where you will get to choose which cluster you would like to login to. \n",
    "\n",
    "## What is a Bastion host?\n",
    "After a successful login, you will be connected to the bastion host. This is a single computer that provides a gateway to our three clusters. This is the only function the bastion host serves. It is not for storing files, running programs, or accessing software. The hostname of this machine is gatekeeper. The Bastion host allows you to connect to different login nodes for each of the clusters we have at UA. You can select from puma, ocelote, or elgato. In our class, we will use ocelote, our teaching cluster. \n",
    "\n",
    "## What is a login node?\n",
    "When you first log into a supercomputer from the Bastion host, you're connected to something called a login node. This is a single computer that's connected to the cluster of compute nodes and is where you write and submit your jobs using a job scheduler. A login node itself is not used for performing any analyses. \n",
    "\n",
    "## What is a job scheduler?\n",
    "A job scheduler is software used to coordinate user jobs. The job scheduler we use is called SLURM. You can use it by writing a special script that requests compute resources (e.g., CPUs, RAM, GPUs) and includes instructions for running your code. You submit this script to the job scheduler using a special command called `sbatch` and it does the work of finding the required space on the supercomputer for you. It then runs your code and returns the results to your account in a text file.\n",
    "\n",
    "## Let's look at an analogy.\n",
    "One way you might think of a supercomputer setup is as a post office and factory. Imagine you have something you need built in a factory and have a list of instructions and materials for how to do it. To achieve this, you put your instructions (code) in an addressed envelope (SLURM script), take it to a post office (login node), have postal worker (job scheduler) deliver the instructions to the factory (compute node), and then you can go home (log off). After waiting for a period of time, your completed project is delivered to you (as a text file).\n",
    "\n",
    "## What is a node?\n",
    "Here is one of the computers, or compute nodes, in Ocelote. They have the same components as a PC or workstation: there are two processor modules, memory DIMMs, an internal disk and networking ports. The power supplies are in the chassis that the compute nodes plug into. Some of the compute nodes have GPUs like the ones in your Xbox or gaming laptop that you use for Minecraft but are much more capable.\n",
    "\n",
    "<a href=\"../fig/ocelote_compute_node.png\">\n",
    "  <img src=\"../fig/ocelote_compute_node.png\" alt=\"A compute node on the ocelote cluster.\"/>\n",
    "</a>\n",
    "\n",
    "## Why should I use an HPC?\n",
    "There are lots of reasons to use a supercomputer! For example, say you have analyses that require a tremendous amount of memory or storage space. It's not feasible (or very expensive) to use 3TB of memory or 10TB of disk space on a local workstation, but on our systems it's very possible (and free). This is how you scale up from the workstation under your desk. \n",
    "\n",
    "<br>\n",
    "Another possibility is you may have thousands of metagenomes to run through quality control. This may take an unreasonable amount of time and be a serious bottleneck for your research if you're running them in serial locally. However, on a supercomputer you can run hundreds of jobs at the same time using thousands of CPUs. This means you may wind up getting results in hours instead of months. This how you scale out your work.\n",
    "\n",
    "<br>\n",
    "You may also have experience with being frustrated with a job's runtime. What happens if it takes a week or longer to complete one of your analyses? On a local workstation, keeping your computer awake for the duration of the run may be difficult, inconvenient, or impossible. On a supercomputer, the process of running jobs can be fully automated. Once you have a special script written with all the necessary instructions, you can submit it to the scheduler and it does the rest. This means you can log out and close your computer without any worry about interrupting your work. Your results are returned to you as a text file in your account in real time so you can always log in and check your progress. You can even request email notifications to keep track of your job's status, though you'll want to be careful not to mail bomb yourself if you're running thousands of jobs.\n",
    "\n",
    "## Where do I store my files?\n",
    "Every compute node in our supercomputers is connected to a large storage array. This storage array is where any files you [upload to the system](https://public.confluence.arizona.edu/display/UAHPC/Transferring+Data) are saved and means that no matter where you are on the system, you'll be able to access your data. There are three locations where your files may go, each with a different size limit. Take a look at ouy hpc [storage page](https://public.confluence.arizona.edu/display/UAHPC/Storage#Storage-Tier1HPCHighPerformanceStorage(Tier1)) for more detailed information.\n",
    "\n",
    "## How do you actually access a compute node?\n",
    "To connect to a compute node, you will need to use the job scheduler. A scheduler, in our case SLURM, is software that will find and reserve resources on a cluster's compute nodes as space becomes available. Resources include things like memory, CPUs, and GPUs that you want to reserve for personal use for a specified period of time. You can use the job scheduler to request two types of jobs: interactive and batch.\n",
    "\n",
    "## Interactive jobs\n",
    "Let's start with a basic interactive job to get a feel for things.\n",
    "Starting a session. To connect to a compute node to work interactively, use the command interactive -a your_group replacing your_group with your own group's name. In our case, we're going to use `bhurwitz`.\n",
    "\n",
    "```\n",
    "interactive -a bh_class\n",
    "```\n",
    "\n",
    "You will see the following output:\n",
    "```\n",
    "Run \"interactive -h for help customizing interactive use\"\n",
    "Submitting with /usr/local/bin/salloc --job-name=interactive --mem-per-cpu=4GB --nodes=1    --ntasks=1 --time=01:00:00 --account=bhurwitz --partition=standard\n",
    "\n",
    "NOTICE: Requesting an interactive session on the elgato system results in \n",
    "significantly shorter wait times. Consider executing the elgato command before \n",
    "using interactive.\n",
    "\n",
    "salloc: Pending job allocation 8060393\n",
    "salloc: job 8060393 queued and waiting for resources\n",
    "\n",
    "```\n",
    "\n",
    "This gives us information about what compute resources we are asking for. The information here is what you would get by default. But, you can customize this and ask for more resources. For example, maybe I want more memory per CPU. First, we need to exit out of the compute node we just went to.\n",
    "\n",
    "```\n",
    "exit\n",
    "interactive -a bh_class -mem-per-cpu=8GB\n",
    "```\n",
    "\n",
    "Check our all of your options to modify the interactive command [here](https://public.confluence.arizona.edu/display/UAHPC/Running+Jobs+with+SLURM#RunningJobswithSLURM-interactive-jobsInteractiveJobs).\n",
    "\n",
    "### Let's check out some software\n",
    "Software packages are **not available on the login nodes** but are available on the compute nodes. Now that we're connected to one, we can see what's available. Software on HPC comes installed as modules. Modules make it easy to load and unload software from your environment. This allows hundreds of packages to be available on the same system without dependency or versioning conflicts. It's always good practice to specify which version of the software you need when loading to ensure a stable environment.\n",
    "\n",
    "You can view and load software modules using the command module avail and module load, respectively.\n",
    "\n",
    "Let's try this out:\n",
    "\n",
    "```\n",
    "module avail\n",
    "```\n",
    "\n",
    "You should see a ton of output that looks like this\n",
    "\n",
    "```\n",
    "(puma) [r4u10n1@/xdisk/bhurwitz]$ module avail\n",
    "\n",
    "-------------------- /opt/ohpc/pub/moduledeps/gnu8-openmpi3 --------------------\n",
    "   abyss/2.2.4                 netcdf-fortran/4.5.2\n",
    "   castep/20.11                netcdf/4.7.1\n",
    "   cp2k-cuda/7.1.0             nufeb/3.0\n",
    "   cp2k/7.1.0                  openfoam/20.06\n",
    "   gromacs/2020.2              parflow/3.7.0\n",
    "   gromacs/2021.5       (D)    parflow/3.9.0        (D)\n",
    "   hwloc/2.2.0                 petsc-complex/3.14.2\n",
    "   hypre/2.11.2                petsc/3.13.1\n",
    "\n",
    "```\n",
    "\n",
    "You can run any of these by doing the following:\n",
    "\n",
    "```\n",
    "module load fastqc\n",
    "fastqc --help\n",
    "```\n",
    "\n",
    "You should see output from the fastqc program describing how to run the program and the flags you can use to change how the program runs.\n",
    "\n",
    "```\n",
    "(puma) [r4u10n1@/xdisk/bhurwitz]$ module load fastqc/0.11.9\n",
    "(puma) [r4u10n1@/xdisk/bhurwitz]$ fastqc --help\n",
    "\n",
    "            FastQC - A high throughput sequence QC analysis tool\n",
    "\n",
    "SYNOPSIS\n",
    "\n",
    "\tfastqc seqfile1 seqfile2 .. seqfileN\n",
    "\n",
    "    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n",
    "           [-c contaminant file] seqfile1 .. seqfileN\n",
    "...\n",
    "```\n",
    "\n",
    "### Benefits of interactive sessions\n",
    "Interactive sessions are excellent development environments. When connected to a compute node, some things you can do are:\n",
    "\n",
    "<li>Run, test, and debug your code\n",
    "<li>View, test, and use software\n",
    "<li>Install your own software\n",
    "<li>Run computationally-intensive commands that might impact others on the login nodes\n",
    "\n",
    "### Drawbacks of interactive sessions\n",
    "Interactive sessions are great testing and development environments, but may not be optimally suited for certain types of analyses. Some issues that may arise include:\n",
    "<li>Your session may time out due to inactivity\n",
    "<li>Your internet connection may get disrupted\n",
    "<li>Your computer may get closed or turned off\n",
    "<li>You want to run more than one job at a time\n",
    "\n",
    "<br>\n",
    "What's a good solution to deal with these challenges? The answer: batch jobs!\n",
    "\n",
    "## Batch jobs\n",
    "### The basics:\n",
    "Batch jobs are a way of submitting work to run on HPC without the need to be present. This means you can log out of the system, turn off your computer and walk away without your work being interrupted. It also means you can submit multiple (up to 1000) jobs to run simultaneously!\n",
    "\n",
    "Running these sorts of jobs requires two steps:\n",
    "\n",
    "Step 1: Create a shell script with three sections: \n",
    "1.\t The header. This is called a shebang and goes in every batch script. It tells the system which interpreter to use (i.e., which language to execute the instructions in):\n",
    "Part 1: The shebang \n",
    "#!/bin/bash\n",
    "2.\tInstructions that tell the job scheduler the resources you need and any other job specifications. This section will look like:\n",
    "Part 2: The SBATCH directives \n",
    "#SBATCH --option1=value1\n",
    "#SBATCH --option2=value2\n",
    ". . .\n",
    "3.\tA blueprint of how to run your work. This includes all the commands you would need to run in the terminal. This section might look like:\n",
    "Part 3: The blueprint \n",
    "cd /path/to/directory\n",
    "module load python/3.9\n",
    "python3 some_script.py\n",
    "\n",
    "Step 2.\tSubmit the shell script to the scheduler with the command sbatch.\n",
    "\n",
    "Let's try creating our first job now using the outline provided above.\n",
    "\n",
    "### Creating the sample shell script\n",
    "Let's start by creating a simple shell script that we'll run in batch.\n",
    "Create a directory and a blank file. And open it using your favorite text editor (nano, in this case)\n",
    "\n",
    "```\n",
    "$ cd /xdisk/bhurwitz/bh_class/your_netid/exercises/03_intro_hpc\n",
    "$ touch hello_world.sh\n",
    "$ nano hello_world.sh\n",
    "``` \n",
    "\n",
    "Now add the following to the file, then save and exit:\n",
    "\n",
    "```\n",
    "echo \"Hello world! I am running on\"; hostname; date\n",
    "```\n",
    "\n",
    "If we run this interactively...\n",
    "\n",
    "```\n",
    "$ bash hello_world.sh\n",
    "```\n",
    "we'll see...\n",
    "\n",
    "```\n",
    "Hello world! I am running on\n",
    "wentletrap.hpc.arizona.edu\n",
    "Mon Sep  4 19:52:05 MST 2023\n",
    "```\n",
    "\n",
    "### Creating the batch script\n",
    "Instead of running this on the login node, we can send the job out to run on the cluster. To do this, we need to create a slurm script with a few more details, that tell the scheduler what resources we need to run the job. Now, let's make a new file called hello_world.slurm, and open it with nano for writing.\n",
    "\n",
    "```\n",
    "$ touch hello_world.slurm\n",
    "$ nano hello_world.slurm\n",
    "```\n",
    "\n",
    "#### Add the shebang and SBATCH directives\n",
    "In this example, we're using the standard partition. A partition is a job queue and affects a job's priority and how many hours are charged. \n",
    "A comprehensive list of all the options you can specify in your batch script can be found on our [Running Jobs with SLURM](https://public.confluence.arizona.edu/display/UAHPC/Running+Jobs+with+SLURM#RunningJobswithSLURM-batch-directivesBatchJobDirectives). \n",
    "\n",
    "In this example, we'll stick with some of the basics:\n",
    "hello_world.slurm \n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "# ------------------------------------------------\n",
    "### PART 1: Requests resources to run your job.\n",
    "# ------------------------------------------------\n",
    "### Optional. Set the job name\n",
    "#SBATCH --job-name=hello_world\n",
    "### Optional. Set the output filename. \n",
    "### SLURM reads %x as the job name and %j as the job ID\n",
    "#SBATCH --output=%x-%j.out\n",
    "### REQUIRED. Specify the PI group for this job. Replace <PI GROUP> with your own group. (bhurwitz for this class)\n",
    "#SBATCH --account=<PI GROUP>\n",
    "### REQUIRED. Set the partition for your job. This is a job queue\n",
    "#SBATCH --partition=standard\n",
    "### REQUIRED. Set the number of nodes\n",
    "#SBATCH --nodes=1\n",
    "### REQUIRED. Set the number of CPUs that will be used for this job.  \n",
    "#SBATCH --ntasks=1 \n",
    "### REQUIRED. Set the memory required for this job. \n",
    "#SBATCH --mem-per-cpu=5gb\n",
    "### REQUIRED. Specify the time required for this job, hhh:mm:ss\n",
    "#SBATCH --time=00:01:00 \n",
    "\n",
    "# --------------------------------------------------\n",
    "### PART 2: Executes bash commands to run your job\n",
    "# ---------------------------------------------------\n",
    "\n",
    "### change to your script’s directory\n",
    "### be sure to change to your netid \n",
    "cd /xdisk/bhurwitz/bh_class/your_netid/exercises/03_intro_hpc\n",
    "\n",
    "### Run your bash commands directly\n",
    "### echo \"Hello world! I am running on\"; hostname; date\n",
    "### sleep 10\n",
    "\n",
    "# Or you can run the bash script you just wrote\n",
    "bash hello_world.sh\n",
    "\n",
    "# or you can run script!\n",
    "# just make sure to load what you need, for example:\n",
    "# module load python/3.9\n",
    "# python3 hello_world.py\n",
    "\n",
    "```\n",
    "Now save and exit.\n",
    "\n",
    "### Submitting the job\n",
    "\n",
    "In this tutorial, we are submitting our job from an interactive session on a compute node. You may also submit jobs from a login node. The only exception is an array job (that we weill use later) that can only be submitted from a login node.\n",
    "\n",
    "The next step is to submit your job request to the scheduler. To do this, you’ll use the command sbatch. This will place your job in line for execution and will return a job ID. This job ID can be used to check your job’s status with squeue, cancel your job with scancel, and get your job’s history with job-history.\n",
    "<br>\n",
    "\n",
    "Let’s run our script and check its status (substitute your own job ID below where relevant):\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "$ sbatch hello_world.slurm \n",
    "```\n",
    "\n",
    "```\n",
    "Submitted batch job 807387\n",
    "```\n",
    "\n",
    "Now, let's check where it is in queue:\n",
    "```\n",
    "squeue --job 807387\n",
    "```\n",
    "\n",
    "```\n",
    "JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "807387  standard hello_wo    netid PD       0:06      1 gpu66\n",
    "```\n",
    "\n",
    "You can see its state is PD (for pending) which means it’s waiting to be executed by the system. Its state will go to R when it’s running and when the job has completed running, squeue will return a blank line.\n",
    "\n",
    "Let’s check the contents of our file with cat. \n",
    "\n",
    "```\n",
    "cat hello_world-807387.out \n",
    "```\n",
    "\n",
    "If your run was successful, you should see:\n",
    "\n",
    "```\n",
    "Hello world! I am running on\n",
    "r2u07n2.puma.hpc.arizona.edu\n",
    "Tue Sep  5 09:40:03 MST 2023\n",
    "```\n",
    "\n",
    "Note that the hostname in this run is different from the hostname of the computer we're connected to. This is because it's a separate job from our interactive session and so may run on any other applicable machines on the cluster.\n",
    "\n",
    "Now you are ready to start working on the HPC!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "df955ce39d0f31d56d4bb2fe0a613e5326ba60723fd33d8303a3aede8f65715c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
