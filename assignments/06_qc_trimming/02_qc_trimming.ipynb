{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de7bac8",
   "metadata": {},
   "source": [
    "# Quality Control and Trimming\n",
    "\n",
    "This notebook will go through the workflow for read quality control and trimming. We will follow each of the steps below, that will require time on the HPC to run. Be sure to check back after each step to make sure you have the right files, and start the next step.   \n",
    "\n",
    "1. Quality control using fastqc to determine quality thresholds.\n",
    "2. Compressing files before trimming.\n",
    "3. Trimming reads with [Trimmomatic](https://carpentries-lab.github.io/metagenomics-analysis/03-trimming-filtering/index.html).\n",
    "4. Optional: Final QC check after trimming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebed36",
   "metadata": {},
   "source": [
    "## Getting Started (Run this before each step)\n",
    "\n",
    "You will need to rerun this section each time you come back to this notebook to kick off the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a46521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the variables for your netid and xfile\n",
    "netid = \"MY_NETID\"\n",
    "xfile = \"MY_XFILE\"\n",
    "file_count = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d388ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into the working directory\n",
    "work_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/assignments/02_qc_trimming\"\n",
    "%cd $work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d323314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the fastq directory\n",
    "fastq_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/project/01_getting_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380dffed",
   "metadata": {},
   "source": [
    "## Creating a config file\n",
    "Each of the scripts below executes code that requires certain variables to be set. So we don't need to edit the code in each of the scripts, we are going to use a config file that defines all of these variables. Then when we want to use these variables in the script, we will \"source\" the config file to set the variables. This is generally a good practice in writing scripts on the HPC, that makes it so you only need to modify the config file (rather than each individual script). We are going to create this file using the variables you set above in \"Getting started\". Note that you only need to create this config file once, even if you are returning to complete the next step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cf341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config file with all of the variables you need\n",
    "!echo \"export NETID=$netid\" > config.sh\n",
    "!echo \"export XFILE=$fastq_dir/$xfile\" >> config.sh\n",
    "!echo \"export FILE_COUNT=$file_count\" >> config.sh\n",
    "!echo \"export FASTQC=/contrib/singularity/shared/bhurwitz/fastqc-0.11.9.sif\" >> config.sh\n",
    "!echo \"export TRIMMOMATIC=/contrib/singularity/shared/bhurwitz/trimmomatic:0.39--hdfd78af_2.sif\" >> config.sh\n",
    "!echo \"export WORK_DIR=$work_dir\" >> config.sh\n",
    "!echo \"export FASTQ_DIR=$fastq_dir\" >> config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the config file to be sure it is correct\n",
    "# Is your netid and xfile correct? Do you have the right directories?\n",
    "!cat config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeaabfe",
   "metadata": {},
   "source": [
    "## Step 1: Assessing Read Quality\n",
    "\n",
    "Now that we have all of our data downloaded, we are ready to start the quality control process. We will use a tool called fastqc that generates a report about the quality of our sequence data.\n",
    "\n",
    "First, we will create an sbatch script that runs fastqc on each of the sequence files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to run fastqc on each of our accessions\n",
    "# A few important points:\n",
    "# 1. We are using the variables from the config file via\n",
    "# the `source ./config.sh` command in the script.\n",
    "# 2. fastqc runs on each of the fastq files in the $FASTQ_DIR\n",
    "# 3. We are creating a directory called check_fastqc in our home directory\n",
    "# this allows us to copy the *html files produced by fastqc and explore\n",
    "# them using Jupyter server on the on demand hpc portal.\n",
    "# 4. The fastqc program runs in the $FASTQ_DIR, but to keep our files\n",
    "# organized, we are going to move the results into our $WORK_DIR.\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-${FILE_COUNT}                        \n",
    "#SBATCH --output=Job-fastqc-%a.out\n",
    "#SBATCH --cpus-per-task=1                  \n",
    "#SBATCH --mem=4G                           \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat $FASTQ_DIR/$XFILE))\n",
    "\n",
    "apptainer run ${FASTQC} fastqc \\\n",
    "    $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq*\n",
    "\n",
    "# create a results directory\n",
    "\n",
    "\n",
    "mkdir ~/check_fastqc\n",
    "cp $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.html ~/check_fastqc \n",
    "mv $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.html $WORK_DIR\n",
    "mv $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.zip $WORK_DIR\n",
    " \n",
    "'''\n",
    "\n",
    "with open('02A_run_fastqc.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b30b51",
   "metadata": {},
   "source": [
    "Great job! \n",
    "\n",
    "Once this step runs, you can check the fastqc files to see the quality of your data. To do this:\n",
    "\n",
    "1. Start up the Jupyter server on HPC on demand, navigate to the folder called check_fastqc\n",
    "2. Double click on each of the html files to check the quality of each of your sequence files. \n",
    "\n",
    "Be sure to refer back to the in-class exercise on quality control to understand what each of the sections means. \n",
    "\n",
    "Because your sequence data come from the SRA, you will likely find that your files are all passing quality control checks already. But, to be certain, we will run a few basic \"screening and cleaning\" steps via trimmomatic in Step 3 to make sure the sequences are up to par. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce6846",
   "metadata": {},
   "source": [
    "## Step 2: Compressing your *fastq files using gzip \n",
    "\n",
    "Trimmomatic works on FASTQ files that are compressed with either gzip or bzip2. So, before we can run trimmomatic, we will need to compress our read files. We'll be using gzip to compress files and get the .gz file extension we need. Because your FASTQ files are large, gzip takes time to run, so let's create a script to sbatch the compression job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a script that gzip's all of the FASTQ files\n",
    "# These are huge files, so it may take 2 hours to run.\n",
    "# This script uses gzip to compress each of the *.fastq files in your fastq_dir.\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-${FILE_COUNT}\n",
    "#SBATCH --output=Job-gzip-%a.out\n",
    "#SBATCH --cpus-per-task=1   \n",
    "#SBATCH --mem=4G                \n",
    " \n",
    "pwd; hostname; date\n",
    "source ./config.sh\n",
    "names=($(cat ${FASTQ_DIR}/${XFILE}))\n",
    "gzip ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq\n",
    "'''\n",
    "\n",
    "with open('02B_run_gzip.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eda481",
   "metadata": {},
   "source": [
    "## Step 3: Trimming .fastq Files\n",
    "\n",
    "In order to run trimmomatic in a PE (paired-end) format we'll need two files. \n",
    "\n",
    "These files are:  *_1.fastq.gz and *_2.fastq.gz for each accession from the SRA. \n",
    "\n",
    "You should now have those from the steps above. Note that we are following the same trimming protocol from the in-class exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf10aee",
   "metadata": {},
   "source": [
    "### Initial Data Management\n",
    "The output from trimmomatic will give us 4 output files (forward paired, forward unpaired, reverse paired and reverse unpaired. To keep our data organized, let's set up some output directories so the script can organize our data as it runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trimmed and unpaired directories\n",
    "import os\n",
    "\n",
    "trim_dir = work_dir + \"/trimmed_reads\"\n",
    "unpair_dir = work_dir + \"/unpaired_reads\"\n",
    "\n",
    "if os.path.isdir(trim_dir):\n",
    "    print(\"trim_dir exists\")\n",
    "else:\n",
    "    os.mkdir(trim_dir)\n",
    "\n",
    "if os.path.isdir(unpair_dir):\n",
    "    print(\"unpair_dir exists\")\n",
    "else:\n",
    "    os.mkdir(unpair_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to copy the adapter file into your current working directory\n",
    "!cp /xdisk/bhurwitz/bh_class/TruSeq3-PE-2.fa .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e65b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a script that runs trimmomatic on all of our fastq files\n",
    "# you can only run this after the *.fastq files are all gzip-ed (step 2)\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-${FILE_COUNT}\n",
    "#SBATCH --output=Job-trim-%a.out\n",
    "#SBATCH --cpus-per-task=1                   \n",
    "#SBATCH --mem=4G                   \n",
    " \n",
    "pwd; hostname; date\n",
    "source ./config.sh\n",
    "names=($(cat ${FASTQ_DIR}/${XFILE}))\n",
    "\n",
    "TRIM_DIR=\"${WORK_DIR}/trimmed_reads\"\n",
    "UNPAIR_DIR=\"${WORK_DIR}/unpaired_reads\"\n",
    "\n",
    "apptainer run ${TRIMMOMATIC} trimmomatic PE -phred33 \\\n",
    "    ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz \\\n",
    "    ${TRIM_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz ${UNPAIR_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz \\\n",
    "    ${TRIM_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz ${UNPAIR_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz \\\n",
    "    ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10 SLIDINGWINDOW:4:20\n",
    "'''\n",
    "\n",
    "with open('02C_run_trimmomatic.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd0bf1",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "Now that you have seen how to create each of the \"run\" scripts for each step above, it is your turn to create a run script to perform the last step described below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545c522",
   "metadata": {},
   "source": [
    "## Step 4 QC Final Check\n",
    "\n",
    "Create a run script that performs a final quality control check, using fastqc, on the trimmed fastq files.\n",
    "\n",
    "This script will use the fastqc tool, and a similar script to the one in 2A, but will check the reads that are in the \"trimmed\" directory. The results should be output to the \n",
    "\n",
    "If you have any doubts about the trimming process, you can always run fastqc on the trimmed data and double check that you see all \"green\". You can check the fastqc files using Jupyter to check for any failures or other warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5eb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are re-running fastqc on the trimmed data\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-4                         \n",
    "#SBATCH --output=Job-fastqc-trim-%a.out\n",
    "#SBATCH --cpus-per-task=1                  \n",
    "#SBATCH --mem=4G                           \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat $FASTQ_DIR/$XFILE))\n",
    "TRIM_DIR=\"${WORK_DIR}/trimmed_reads\"\n",
    "\n",
    "apptainer run /contrib/singularity/shared/bhurwitz/fastqc-0.11.9.sif fastqc \\\n",
    "    $TRIM_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq*\n",
    "\n",
    "mkdir ~/check_fastqc_trimmed2\n",
    "cp $TRIM_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.html ~/check_fastqc_trimmed2 \n",
    "\n",
    "'''\n",
    "\n",
    "with open('fastqc_trim_parallel.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ca9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch ./fastqc_trim_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d039345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if your job is finished running\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688651d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89fb4e38",
   "metadata": {},
   "source": [
    "## Step 3: Putting it all together\n",
    "\n",
    "Once you have created the the run scripts, you are ready to put them together in a pipeline to run each of the steps one by one.\n",
    "\n",
    "Note that 01A_run_prefetch jobs need to finish, before we can kick off the 01B_run_fasterq-dump. To do this, we will need to set up dependencies in our \"launch script\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d650f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the launcher script to kick off our pipeline.\n",
    "\n",
    "my_code = '''                     \n",
    " \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "with open('02_launch_pipeline.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e001991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's run it!\n",
    "!sbatch ./02_launch_pipeline.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if it is running using the squeue command\n",
    "# Check for all jobs under your netid\n",
    "# Notice that 02B jobs are dependent on 02A jobs finishing and etc.\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd60c37",
   "metadata": {},
   "source": [
    "Once your job completes, you can look at the *.html files in your home directory in ~/check_fastqc_trimmed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5e252",
   "metadata": {},
   "source": [
    "### Data Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check that all of your files have run through fastqc.\n",
    "# Do you see a *.html and *.zip file for each one?\n",
    "!ls /xdisk/bhurwitz/bh_class/$netid/assignments/06_qc_trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fcb27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if all of your *.fastq files are gzip-ed\n",
    "# Note that these files are in the \"05_getting_data\" directory\n",
    "# You should just see one gzipped file for each fastq\n",
    "# For example, ERR2198631_1.fastq.gz not ERR2198631_1.fastq too...\n",
    "# If you see both .fastq and .fastq.gz the gzip command \n",
    "# is still in progress.\n",
    "!ls /xdisk/bhurwitz/bh_class/$netid/assignments/05_getting_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315ab967",
   "metadata": {},
   "source": [
    "### Checking your output files\n",
    "\n",
    "Once your job has completed, you should see that there are four output files from two input files. The trimmomatic program places all of the \"orphaned\" reads in a separate file from the trimmed reads. Reads can become orphaned when their \"mate pair\" is either too short, or too low quality. For our analyses going forward, we will only use the reads that were trimmed, and have both the forward and reverse read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the file sizes to see that they are smaller for our trimmed\n",
    "# reads\n",
    "!echo \"trimmed:\"\n",
    "!ls -l /xdisk/bhurwitz/bh_class/$netid/assignments/06_qc_trimming/trimmed_reads/*fastq.gz\n",
    "!echo \"untrimmed:\"\n",
    "!ls -l /xdisk/bhurwitz/bh_class/$netid/assignments/05_getting_data/*fastq.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83501ab5",
   "metadata": {},
   "source": [
    "## Final Step\n",
    "Copy your notebook to the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp ~/06_qc_trimming.ipynb $work_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
