{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_getting_data\n",
    "\n",
    "### Downloading FASTQ files from the SRA\n",
    "\n",
    "Now comes the exciting part. We are going to get started on your project. The first step in this process is to download data from the Sequence Read Archive or SRA. This notebook will walk you through all of the steps in this process using a tool called the sra-toolkit. \n",
    "\n",
    "-----------\n",
    "\n",
    "Sections:\n",
    "\n",
    "1. Pre-fetching your FASTQ files\n",
    "2. Using fasterq-dump to download the FASTQ R1 and R2 (forward and reverse) read files\n",
    "3. Checking that your FASTQ files have been downloaded\n",
    "\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Before we get started you will need to set several variables that we will use throughout this notebook. \n",
    "\n",
    "You will need to rerun this section each time you come back to this notebook to reset the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the variables for your netid and xfile\n",
    "netid = \"MY_NETID\"\n",
    "xfile = \"MY_XFILE\"\n",
    "file_count = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and change into this directory\n",
    "work_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/project/01_getting_data\"\n",
    "%cd $work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the fastq directory\n",
    "fastq_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/project/01_getting_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a config file\n",
    "The scripts below executes code that requires certain variables to be set. So we don't need to edit the code in the script, we are going to use a config file that defines all of these variables for us. Then when we want to use these variables in the script, we will \"source\" the config file to set the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config file with all of the variables you need\n",
    "!echo \"export NETID=$netid\" > config.sh\n",
    "!echo \"export XFILE=$fastq_dir/$xfile\" >> config.sh\n",
    "!echo \"export FILE_COUNT=$file_count\" >> config.sh\n",
    "!echo \"export SRA_TOOLKIT=/contrib/singularity/shared/bhurwitz/sra-tools-3.0.3.sif\" >> config.sh\n",
    "!echo \"export WORK_DIR=$work_dir\" >> config.sh\n",
    "!echo \"export FASTQ_DIR=$fastq_dir\" >> config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prefetching the FASTQ files for your project\n",
    "\n",
    "The very first step in downloading data from the Sequence Read Archive (SRA) at NCBI is to \"pre-fetch\" the data using the SRA toolkit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using containers to run bioinformatics tools\n",
    "\n",
    "We will be running many bioinformatics tools using containers. Containers are virtual environments that contain all of the necessary components to run the code. This includes the operating system, the tool, and any dependencies. Containers allow programmers to \"package\" up their code, so it can be run anywhere (on a laptop, HPC, or in the cloud) without having to reinstall and set everything up to run the code there locally. Everything is in the container!\n",
    "\n",
    "The UA HPC requires us to use the apptainer command to create/run our bioinformatics tools in containers. The command to run a container looks something like this:\n",
    "\n",
    "```\n",
    "apptainer run ${SRA_TOOLKIT} prefetch [options and files]\n",
    "\n",
    "```\n",
    "\n",
    "Because the apptainer command can only be run from one of the compute nodes, not the login node, we have to put this code inside shell script to run it. We then use the sbatch command to \"launch\" this script on the HPC.\n",
    "\n",
    "It looks something like this...\n",
    "\n",
    "```\n",
    "sbatch run_script.sh\n",
    "```\n",
    "\n",
    "OK, let's get started by creating our \"run_scripts\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the run_script to pre-fetch fastq files by using Python to write it for us.\n",
    "\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-${FILE_COUNT}                          \n",
    "#SBATCH --output=01A_run_prefetch-%a.out\n",
    "#SBATCH --error=01A_run_prefetch-%a.err\n",
    "#SBATCH --cpus-per-task=4                    \n",
    "#SBATCH --mem-per-cpu=2G                            \n",
    " \n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat ${FASTQ_DIR}/${XFILE}))\n",
    " \n",
    "echo ${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "apptainer run ${SRA_TOOLKIT} prefetch ${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "'''\n",
    "\n",
    "with open('01A_run_prefetch.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Downloading FASTQ files for your project\n",
    "\n",
    "Now that you have pre-fetched all of your FASTQ files, we are ready to download them. \n",
    "\n",
    "We will use the fasterq-dump command to get the FASTQ R1 and R2 files. \n",
    "\n",
    "Here's the fasterq-dump [documentation](https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the script to get the FASTQ files by using Python to write it for us.\n",
    "\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-${FILE_COUNT}                          \n",
    "#SBATCH --output=01B_run_fasterq-dump-%a.out\n",
    "#SBATCH --error=01B_run_fasterq-dump-%a.err\n",
    "#SBATCH --cpus-per-task=4                    \n",
    "#SBATCH --mem-per-cpu=2G                            \n",
    " \n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat ${FASTQ_DIR}/${XFILE}))\n",
    " \n",
    "echo ${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "apptainer run ${SRA_TOOLKIT} fasterq-dump --split-files ${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "'''\n",
    "\n",
    "with open('01B_run_fasterq-dump.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compressing the FASTQ files for your project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a script that gzip's all of the FASTQ files\n",
    "# These are huge files, so it may take 2 hours to run.\n",
    "# This script uses gzip to compress each of the *.fastq files in your fastq_dir.\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-${FILE_COUNT}\n",
    "#SBATCH --output=Job-gzip-%a.out\n",
    "#SBATCH --cpus-per-task=1   \n",
    "#SBATCH --mem=4G                \n",
    " \n",
    "pwd; hostname; date\n",
    "source ./config.sh\n",
    "names=($(cat ${FASTQ_DIR}/${XFILE}))\n",
    "gzip ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq\n",
    "'''\n",
    "\n",
    "with open('01C_run_gzip.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Putting it all together\n",
    "\n",
    "Once you have created the the run scripts, you are ready to put them together in a pipeline to run each of the steps one by one.\n",
    "\n",
    "Note that 01A_run_prefetch jobs need to finish, before we can kick off the 01B_run_fasterq-dump. To do this, we will need to set up dependencies in our \"launch script\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the launcher script to kick off our pipeline.\n",
    "\n",
    "my_code = '''                     \n",
    " \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "with open('01_launch_pipeline.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's run it!\n",
    "!sbatch ./01_launch_pipeline.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if it is running using the squeue command\n",
    "# Check for all jobs under your netid\n",
    "# Notice that 01B jobs are dependent on 01A jobs finishing.\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Checking your FASTQ files\n",
    "\n",
    "Your code will take a little time to get \"picked up\" by the HPC and move from PD (pending) to R (running). Be sure to come back and check your directory to be sure that you have R1 and R2 files for each of your accessions. You can run this by returning to this notebook, or by using the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to into the directory you downloaded your FASTQ data\n",
    "netid = \"YOUR_NETID\"\n",
    "%cd /xdisk/bhurwitz/bh_class/$netid/project/01_getting_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if you have an R1 and R2 file for each of your accessions (10 total). Do they have a size > 0?\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the files to see if you have FASTQ formatted data.\n",
    "!head -4 ERR*.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great Job! Be sure to copy your notebook to your work directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp ~/01_getting_data.ipynb /xdisk/bhurwitz/bh_class/$netid/project/01_getting_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "df955ce39d0f31d56d4bb2fe0a613e5326ba60723fd33d8303a3aede8f65715c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
