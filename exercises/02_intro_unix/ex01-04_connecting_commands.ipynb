{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting Commands in Unix\n",
    "\n",
    "### Questions:\n",
    "- How can you use pipes to connect commands and perfrom powerful functions in Unix?\n",
    "\n",
    "### Objectives:\n",
    "- Learn how to connect Unix commands to analyze and refine data.\n",
    "\n",
    "### Keypoints:\n",
    "\n",
    "- Unix commands can be put together in powerful ways.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Finding the Number of Unique Users\n",
    "\n",
    "Find the number of unique users on a shared system\n",
    "\n",
    "We know that \"w\" will tell us the users logged in.  Try it now on a system that has many users \\(i.e., not your laptop\\) and see the output.  We'll connect the output of \"w\" to \"head\" using a pipe \"\\|\", but we only want the first five lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Type the commands below, and run the cell\n",
    "!w | head -5\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you get?\n",
    "\n",
    "You should get something that looks like this...\n",
    "\n",
    "```\n",
    "$ w | head -5\n",
    " 14:36:01 up 21 days, 21:51, 176 users,  load average: 3.83, 4.31, 4.47\n",
    "USER     TTY      FROM              LOGIN@   IDLE   JCPU   PCPU WHAT\n",
    "antontre pts/1    149.165.156.129  Sat14    3days  0.10s  0.10s /bin/sh -i\n",
    "huddack  pts/3    128.4.131.189    09:38    4:56m  0.15s  0.15s -bash\n",
    "```\n",
    "\n",
    "What if we want to see the first five _users_, not the first five lines of output.  To skip the first two lines of headers from \"w,\" we first pipe \"w\" into \"awk\" and tell it we only want to see output when the Number of Records \\(NR\\) is greater than 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Type the commands below, and run the cell\n",
    "!w | awk 'NR>2' | head -5\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should see this...\n",
    "\n",
    "```\n",
    "antontre pts/1    149.165.156.129  Sat14    4days  0.10s  0.10s /bin/sh -i\n",
    "huddack  pts/3    128.4.131.189    09:38    5:13m  0.15s  0.15s -bash\n",
    "antontre pts/5    149.165.156.129  Sun19    2days  0.14s  0.14s /bin/sh -i\n",
    "minyard  pts/8    129.114.64.18    29Jul16  4:24m  3:46m  3:46m top\n",
    "antontre pts/11   149.165.156.129  Sun23    2days  0.24s  0.24s /bin/sh -i\n",
    "```\n",
    "\n",
    "Let's \"cut\" out just the first column of data.  The manpage for \"cut\" says that it defaults to using the tab character to determine columns, so we'll need to tell it to use spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Type the commands below, and run the cell\n",
    "!w | awk 'NR>2' | head -5 | cut -d ' ' -f 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should see this...\n",
    "\n",
    "```\n",
    "antontre\n",
    "huddack\n",
    "antontre\n",
    "minyard\n",
    "antontre\n",
    "```\n",
    "\n",
    "We can see right away that the some users like \"antontre\" are logged in multiple times, so let's \"uniq\" that output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Type the commands below, and run the cell\n",
    "!w | awk 'NR>2' | head -5 | cut -d ' ' -f 1 | uniq\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    "$ w | awk 'NR>2' | head -5 | cut -d ' ' -f 1 | uniq\n",
    "antontre\n",
    "huddack\n",
    "antontre\n",
    "minyard\n",
    "antontre\n",
    "```\n",
    "\n",
    "Hmm, that's not right.  Remember I said earlier that \"uniq\" only works _on sorted input_?  So let's sort those names first:\n",
    "\n",
    "```\n",
    "$ w | awk 'NR>2' | head -5 | cut -d ' ' -f 1 | sort | uniq\n",
    "antontre\n",
    "huddack\n",
    "minyard\n",
    "```\n",
    "\n",
    "OK, that is correct.  Now let's remove the \"head -5\" and use \"wc\" to count all the lines \\(-l\\) of input:\n",
    "\n",
    "```\n",
    "$ w | awk 'NR>2' | cut -d ' ' -f 1 | sort | uniq | wc -l\n",
    "138\n",
    "```\n",
    "\n",
    "So what you see is that we're connecting small, well-defined programs together using pipes to connect the \"standard input\" \\(STDIN\\) and \"standard output \\(STDOUT\\) streams.  There's a third basic file handle in Unix called \"standard error\" \\(STDERR\\) that we'll come across later.  It's a way for programs to report problems without simply dying.  You can redirect errors into a file like so:\n",
    "\n",
    "```\n",
    "$ program 2>err\n",
    "$ program 1>out 2>err\n",
    "```\n",
    "\n",
    "The first example puts STDERR into a file called \"err\" and lets STDOUT print to the terminal.  The second example captures STDOUT into a file called \"out\" while STDERR goes to \"err.\"\n",
    "\n",
    "> Protip: Sometimes a program will complain about things that you cannot fix, e.g., \"find\" may complain about file permissions that you don't care about.  In those cases, you can redirect STDERR to a special filehandle called \"/dev/null\" where they are forgotten forever.  Kind of like the \"memory hole\" in 1984.\n",
    "\n",
    "```\n",
    "find / -name my-file.txt 2>/dev/null\n",
    "```\n",
    "\n",
    "## Count \"oo\" words\n",
    "\n",
    "On almost every Unix system, you can find \"/usr/share/dict/words.\"  Let's use \"grep\" to find how many have the \"oo\" vowel combination.  It's a long list, so I'll pipe it into \"head\" to see just the first five:\n",
    "\n",
    "```\n",
    "$ grep 'oo' /usr/share/dict/words | head -5\n",
    "abloom\n",
    "aboon\n",
    "aboveproof\n",
    "abrood\n",
    "abrook\n",
    "```\n",
    "\n",
    "Yes, that works, so redirect those words into a file and count them:\n",
    "\n",
    "```\n",
    "$ grep 'oo' /usr/share/dict/words > oo-words\n",
    "$ wc -l !$\n",
    "10460 oo-words\n",
    "```\n",
    "\n",
    "Let's count them directly out of \"grep\":\n",
    "\n",
    "```\n",
    "$ grep 'oo' /usr/share/dict/words | wc -l\n",
    "10460\n",
    "```\n",
    "\n",
    "How many of those words additionally contain the \"ow\" sequence?\n",
    "\n",
    "```\n",
    "$ grep 'oo' /usr/share/dict/words | grep 'ow' | wc -l\n",
    "158\n",
    "```\n",
    "\n",
    "How many _do not_ contain the \"ow\" sequence?\n",
    "\n",
    "```\n",
    "$ grep 'oo' /usr/share/dict/words | grep -v 'ow' | wc -l\n",
    "10302\n",
    "```\n",
    "\n",
    "Do those numbers add up?\n",
    "\n",
    "```\n",
    "$ bc <<< 158+10302\n",
    "10460\n",
    "```\n",
    "\n",
    "Excellent.  Smithers, massage my brain.\n",
    "\n",
    "## Gapminder\n",
    "\n",
    "Do the following:\n",
    "\n",
    "```\n",
    "$ git clone https://github.com/kyclark/metagenomics-book\n",
    "$ cd metagenomics-book/problems/gapminder/data\n",
    "```\n",
    "\n",
    "How many files are in the \"data\" directory?\n",
    "\n",
    "```\n",
    "$ ls | wc -l\n",
    "```\n",
    "\n",
    "How many lines are in each/all of the files?\n",
    "\n",
    "```\n",
    "$ wc -l *\n",
    "```\n",
    "\n",
    "You can use `cat` to spew at the entire contents of a file into your shell, but if you'd just like to see the top of a file, you can use:\n",
    "\n",
    "```\n",
    "$ head Trinidad_and_Tobago.cc.txt\n",
    "```\n",
    "\n",
    "If you only want to see 5 lines, use `-n 5` or `-5` .\n",
    "\n",
    "For our exercise, we'd like to combine all the files into one file we can analyze.  That's easy enough with:\n",
    "\n",
    "```\n",
    "$ cat * > all.txt\n",
    "```\n",
    "\n",
    "Let's use `head` to look at the top of file:\n",
    "\n",
    "```\n",
    "$ head -5 all.txt\n",
    "Afghanistan    1997    22227415    Asia    41.763    635.341351\n",
    "Afghanistan    2002    25268405    Asia    42.129    726.7340548\n",
    "Afghanistan    2007    31889923    Asia    43.828    974.5803384\n",
    "Afghanistan    1952    8425333    Asia    28.801    779.4453145\n",
    "Afghanistan    1957    9240934    Asia    30.332    820.8530296\n",
    "```\n",
    "\n",
    "Hmm, there are no column headers.  Let's fix that.  There's one file that's pretty different in content \\(it has only one line\\) and name \\(\"country.cc.txt\"\\):\n",
    "\n",
    "```\n",
    "$ cat country.cc.txt\n",
    "country    year    pop    continent    lifeExp    gdpPercap\n",
    "```\n",
    "\n",
    "Those are the headers that you can combine to all the other files to get named columns, something very important if you want to look at the data in Excel and R/Python data frames.\n",
    "\n",
    "```\n",
    "$ rm all.txt\n",
    "$ mv country.cc.txt headers\n",
    "$ cat headers *.txt > all.txt\n",
    "$ head -5 all.txt | column -t\n",
    "country      year  pop       continent  lifeExp  gdpPercap\n",
    "Afghanistan  1997  22227415  Asia       41.763   635.341351\n",
    "Afghanistan  2002  25268405  Asia       42.129   726.7340548\n",
    "Afghanistan  2007  31889923  Asia       43.828   974.5803384\n",
    "Afghanistan  1952  8425333   Asia       28.801   779.4453145\n",
    "```\n",
    "\n",
    "Yes, that looks much better.  Double-check that the number of lines in the `all.txt` match the number of lines of input:\n",
    "\n",
    "```\n",
    "$ wc -l *.cc.txt headers\n",
    "$ wc -l all.txt\n",
    "```\n",
    "\n",
    "How many observations do we have for 1952?\n",
    "\n",
    "```\n",
    "$ grep 1952 all.txt | wc -l\n",
    "$ cut -f 2 *.cc.txt | grep 1952 | wc -l\n",
    "```\n",
    "\n",
    "Those numbers aren't the same!  Why is that?\n",
    "\n",
    "```\n",
    "$ grep 1952 all.txt | cut -f 2 | sort | uniq -c\n",
    " 142 1952\n",
    "   1 1982\n",
    "   1 1987\n",
    "$ grep 1952 all.txt | grep 198[27]\n",
    "Lebanon    1982    3086876    Asia    66.983    7640.519521\n",
    "Mozambique    1987    12891952    Africa    42.861    389.8761846\n",
    "```\n",
    "\n",
    "How many observations for every year?\n",
    "\n",
    "```\n",
    "$ cut -f 2 *.cc.txt | sort | uniq -c\n",
    "```\n",
    "\n",
    "How many observations are present for Africa?\n",
    "\n",
    "```\n",
    "$ grep Africa all.txt | wc -l\n",
    "```\n",
    "\n",
    "How many for each continent?\n",
    "\n",
    "```\n",
    "$ cut -f 4 *.cc.txt | sort | uniq -c\n",
    "```\n",
    "\n",
    "What was the world population in 1952?  As we've seen, just using `grep 1952` is not sufficient.  We want to take the 3rd column if the 2nd column is equal to \"1952.\"  `awk` will let us do just that.  Normally `awk` will split on whitespace, so we need to use `-F\"\\t\"` to tell it to split on the tab \\(`\\t`\\) character.  Use `man awk` to learn more.\n",
    "\n",
    "```\n",
    "$ awk -F\"\\t\" '$2 == \"1952\" { print $3 }' *.cc.txt\n",
    "```\n",
    "\n",
    "I'll bet you didn't notice that one of those numbers was in scientific notation.  That's going to cause a problem.  Here it is:\n",
    "\n",
    "```\n",
    "$ awk -F\"\\t\" '$2 == \"1952\" { print $3 }' *.cc.txt | grep [a-z]\n",
    "3.72e+08\n",
    "```\n",
    "\n",
    "We have to throw in a `grep -v` to get rid of that \\(the `-v` reverses the match\\), then use the `paste` command is used to put a \"+\" in between all the numbers:\n",
    "\n",
    "```\n",
    "$ awk -F\"\\t\" '$2 == \"1952\" { print $3 }' *.cc.txt | grep -v [a-z] | paste -sd+ -\n",
    "```\n",
    "\n",
    "and then we pipe that to the `bc` calculator:\n",
    "\n",
    "```\n",
    "$ awk -F\"\\t\" '$2 == \"1952\" { print $3 }' *.cc.txt | grep -v [a-z] | paste -sd+ - | bc\n",
    "2034957150.999989\n",
    "```\n",
    "\n",
    "It bothers me that it's not an integer, so I'm going to use `printf` in the `awk` command to trim that:\n",
    "\n",
    "```\n",
    "$ awk -F\"\\t\" '$2 == \"1952\" { printf \"%d\\n\", $3 }' *.cc.txt | grep -v [a-z] | paste -sd+ - | bc\n",
    "2406957150\n",
    "```\n",
    "\n",
    "How did population change over the years?  Let's put a list of the unique years into a file called \"years\" and then `cat` over that to run the above for each year:\n",
    "\n",
    "    $ cut -f 2 *.txt | sort | uniq > years\n",
    "    $ for year in `cat years`; do echo -n $year \": \" && awk -F\"\\t\" \"\\$2 == $year { printf \\\"%d\\n\\\", \\$3 }\" *.cc.txt | grep -v [a-z] | paste -sd+ - | bc; done\n",
    "    1952 : 2406957150\n",
    "    1957 : 2664404580\n",
    "    1962 : 2899782974\n",
    "    1967 : 3217478384\n",
    "    1972 : 3576977158\n",
    "    1977 : 3930045807\n",
    "    1982 : 4289436840\n",
    "    1987 : 4691477418\n",
    "    1992 : 5110710260\n",
    "    1997 : 5515204472\n",
    "    2002 : 5886977579\n",
    "    2007 : 6251013179\n",
    "\n",
    "That's kind of useful!  Here's how I might put that into a script:\n",
    "\n",
    "```\n",
    "$ cat pop-years.sh\n",
    "#!/bin/bash\n",
    "\n",
    "set -u\n",
    "\n",
    "YEARS=\"years\"\n",
    "\n",
    "cut -f 2 ./*.cc.txt | sort | uniq > \"$YEARS\"\n",
    "NUM=$(wc -l $YEARS | awk '{print $1}')\n",
    "\n",
    "if [[ \"$NUM\" -lt 1 ]]; then\n",
    "  echo \"No years ($NUM)!\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "while read -r YEAR; do\n",
    "    echo -n \"$YEAR: \"\n",
    "    awk -F\"\\t\" \"\\$2 == $YEAR { printf \\\"%d\\\\n\\\", \\$3 }\" ./*.cc.txt | grep -v \"[a-z]\" | paste -sd+ - | bc\n",
    "done < \"$YEARS\"\n",
    "$ ./pop-years.sh\n",
    "1952: 2406957150\n",
    "1957: 2664404580\n",
    "1962: 2899782974\n",
    "1967: 3217478384\n",
    "1972: 3576977158\n",
    "1977: 3930045807\n",
    "1982: 4289436840\n",
    "1987: 4691477418\n",
    "1992: 5110710260\n",
    "1997: 5515204472\n",
    "2002: 5886977579\n",
    "2007: 6251013179\n",
    "```\n",
    "\n",
    "How has life expectancy changed over the years?  For this we'll need to write a little Python program.  I'll `cat` the program so you can see it.  You can type this in with `nano` and then do `chmod +x avg.py` to make it executable \\(or use the one I added\\):\n",
    "\n",
    "    $ cat avg.py\n",
    "    #!/usr/bin/env python3\n",
    "\n",
    "    import sys\n",
    "\n",
    "    args = list(map(float, sys.argv[1:]))\n",
    "    print(str(sum(args) // len(args)))\n",
    "    $ for year in `cat years`; do echo -n \"$year: \" && grep $year *.txt | cut -f 5 | xargs ./avg.py; done\n",
    "    1952: 49.0\n",
    "    1957: 51.0\n",
    "    1962: 53.0\n",
    "    1967: 55.0\n",
    "    1972: 57.0\n",
    "    1977: 59.0\n",
    "    1982: 61.0\n",
    "    1987: 63.0\n",
    "    1992: 64.0\n",
    "    1997: 65.0\n",
    "    2002: 65.0\n",
    "    2007: 66.0\n",
    "\n",
    "How many observations where the life expectancy \\(\"lifeExp,\" field \\#5\\) is greater than 40?  For this, let's use the `awk` tool.  \n",
    "\n",
    "```\n",
    "$ awk -F\"\\t\" '$5 > 40' all.txt | wc -l\n",
    "```\n",
    "\n",
    "How many of those are from Africa?  We can either use `cut` to get the 4th field or ask `awk` to print the 4th field for us:\n",
    "\n",
    "```\n",
    "$ awk -F\"\\t\" '$5 > 40' all.txt | cut -f 4 | grep Africa | wc -l\n",
    "$ awk -F\"\\t\" '$5 > 40 {print $4}' all.txt | grep Africa | wc -l\n",
    "```\n",
    "\n",
    "How many countries had a life expectancy greater than 70, grouped by year?\n",
    "\n",
    "```\n",
    "$ awk -F\"\\t\" '$5 > 70 { print $2 }' *.cc.txt | sort | uniq -c\n",
    "   5 1952\n",
    "   9 1957\n",
    "  16 1962\n",
    "  25 1967\n",
    "  30 1972\n",
    "  38 1977\n",
    "  44 1982\n",
    "  49 1987\n",
    "  54 1992\n",
    "  65 1997\n",
    "  75 2002\n",
    "  83 2007\n",
    "```\n",
    "\n",
    "How could we add continent to this?\n",
    "\n",
    "```\n",
    "$ awk -F\"\\t\" '$5 > 70 { print $2 \":\" $4 }' *.cc.txt | sort | uniq -c\n",
    "```\n",
    "\n",
    "As you look at the data and want to ask more complicated questions like how does `gdpPercap` affect `lifeExp`, you'll find you need more advanced tools like Python or R.  Now that the data has been collated and the columns named, that will be much easier.\n",
    "\n",
    "What if we want to add headers to each of the files?\n",
    "\n",
    "```\n",
    "$ mkdir wheaders\n",
    "$ for file in *.txt; do cat headers $file > wheaders/$file; done\n",
    "$ wc -l wheaders/* | head -5\n",
    "      13 wheaders/Afghanistan.cc.txt\n",
    "      13 wheaders/Albania.cc.txt\n",
    "      13 wheaders/Algeria.cc.txt\n",
    "      13 wheaders/Angola.cc.txt\n",
    "      13 wheaders/Argentina.cc.txt\n",
    "$ head wheaders/Vietnam.cc.txt\n",
    "country    year    pop    continent    lifeExp    gdpPercap\n",
    "Vietnam    1952    26246839    Asia    40.412    605.0664917\n",
    "Vietnam    1957    28998543    Asia    42.887    676.2854478\n",
    "Vietnam    1962    33796140    Asia    45.363    772.0491602\n",
    "Vietnam    1967    39463910    Asia    47.838    637.1232887\n",
    "Vietnam    1972    44655014    Asia    50.254    699.5016441\n",
    "Vietnam    1977    50533506    Asia    55.764    713.5371196\n",
    "Vietnam    1982    56142181    Asia    58.816    707.2357863\n",
    "Vietnam    1987    62826491    Asia    62.82    820.7994449\n",
    "Vietnam    1992    69940728    Asia    67.662    989.0231487\n",
    "```\n",
    "\n",
    "## Something with sequences\n",
    "\n",
    "Now we will get some sequence data from the iMicrobe FTP site.  Both \"wget\" and \"ncftpget\" will do the trick:\n",
    "\n",
    "```\n",
    "$ mkdir -p ~/work/abe487/contigs\n",
    "$ cd !$\n",
    "$ wget ftp://ftp.imicrobe.us/abe487/contigs/contigs.zip\n",
    "```\n",
    "\n",
    "> Protip: How do we know we got the correct data?  Go back and look at that FTP site, and you will see that there is a \"contigs.zip.md5\" file that we can \"less\" on the server to view the contents:\n",
    "\n",
    "```\n",
    "$ ncftp ftp://ftp.imicrobe.us/abe487/contigs\n",
    "NcFTP 3.2.5 (Feb 02, 2011) by Mike Gleason (http://www.NcFTP.com/contact/).\n",
    "Connecting to 128.196.131.100...\n",
    "Welcome to the imicrobe.us repository\n",
    "Logging in...\n",
    "Login successful.\n",
    "Logged in to ftp.imicrobe.us.\n",
    "Current remote directory is /abe487/contigs.\n",
    "ncftp /abe487/contigs > ls\n",
    "contigs.zip        contigs.zip.md5\n",
    "ncftp /abe487/contigs > less contigs.zip.md5\n",
    "\n",
    "1b7e58177edea28e6441843ddc3a68ab  contigs.zip\n",
    "ncftp /abe487/contigs > exit\n",
    "```\n",
    "\n",
    "> You can read up on MD5 \\([https://en.wikipedia.org/wiki/Md5sum](https://en.wikipedia.org/wiki/Md5sum)\\) to understand that this is a signature of the file.  If we calculate the MD5 of the file we dowloaded and it matches what we see on the server, then we can be sure that we have the exact file that is on the FTP site:\n",
    "\n",
    "```\n",
    "$ md5sum contigs.zip\n",
    "1b7e58177edea28e6441843ddc3a68ab  contigs.zip\n",
    "```\n",
    "\n",
    "> Yes, those two sums match.  Note that sometimes the program is just called \"md5.\"\n",
    "\n",
    "So, back to the exercise.  Let's unpack the contigs:\n",
    "\n",
    "```\n",
    "$ unzip contigs.zip\n",
    "Archive:  contigs.zip\n",
    "  inflating: group12_contigs.fasta\n",
    "  inflating: group20_contigs.fasta\n",
    "  inflating: group24_contigs.fasta\n",
    "$ rm contigs.zip\n",
    "```\n",
    "\n",
    "These files are in FASTA format \\([https://en.wikipedia.org/wiki/FASTA\\_format](https://en.wikipedia.org/wiki/FASTA_format)\\), which basically looks like this:\n",
    "\n",
    "```\n",
    ">MCHU - Calmodulin - Human, rabbit, bovine, rat, and chicken\n",
    "ADQLTEEQIAEFKEAFSLFDKDGDGTITTKELGTVMRSLGQNPTEAELQDMINEVDADGNGTID\n",
    "FPEFLTMMARKMKDTDSEEEIREAFRVFDKDGNGYISAAELRHVMTNLGEKLTDEEVDEMIREA\n",
    "DIDGDGQVNYEEFVQMMTAK*\n",
    ">gi|5524211|gb|AAD44166.1| cytochrome b [Elephas maximus maximus]\n",
    "LCLYTHIGRNIYYGSYLYSETWNTGIMLLLITMATAFMGYVLPWGQMSFWGATVITNLFSAIPYIGTNLV\n",
    "EWIWGGFSVDKATLNRFFAFHFILPFTMVALAGVHLTFLHETGSNNPLGLTSDSDKIPFHPYYTIKDFLG\n",
    "LLILILLLLLLALLSPDMLGDPDNHMPADPLNTPLHIKPEWYFLFAYAILRSVPNKLGGVLALFLSIVIL\n",
    "GLMPFLHTSKHRSMMLRPLSQALFWTLTMDLLTLTWIGSQPVEYPYTIIGQMASILYFSIILAFLPIAGX\n",
    "IENY\n",
    "```\n",
    "\n",
    "Header lines start with \"&gt;\", then the sequence follows.  Sequences may be broken up over several lines of 50 or 80 characters, but it's just as common to see the sequences take only one \\(sometimes very long\\) line.  Sequences may be nucleotides, proteins, very short DNA/RNA, longer contigs \\(shorter strands assembled into contiguous regions\\), or entire chromosomes or even genomes.\n",
    "\n",
    "So, how many sequences are in \"group12\\_contigs.fasta\"?  To answer, we just need to count how many times we see \"&gt;\".  We can do that with \"grep\":\n",
    "\n",
    "```\n",
    "$ grep > group12_contigs.fasta\n",
    "Usage: grep [OPTION]... PATTERN [FILE]...\n",
    "Try 'grep --help' for more information.\n",
    "```\n",
    "\n",
    "What is going on?  Remember when we captured the \"oo\" words that we used the \"&gt;\" symbol to tell Unix to _redirect_ the output of \"grep\" into a file.  We need to tell Unix that we mean a literal greater-than sign by placing it in single or double quotes or putting a backslash in front of it:\n",
    "\n",
    "```\n",
    "$ grep '>' group12_contigs.fasta\n",
    "$ grep \\> group12_contigs.fasta\n",
    "```\n",
    "\n",
    "You should actually see nothing because something quite insidious happened with that first \"grep\" statement -- it overwrote our original \"group12\\_contigs.fasta\" with the result of \"grep\"ing for nothing, which is nothing:\n",
    "\n",
    "```\n",
    "$ ls -l group12_contigs.fasta\n",
    "-rw-rw---- 1 kyclark staff 0 Aug 10 15:08 group12_contigs.fasta\n",
    "```\n",
    "\n",
    "Ugh, OK, I have to go back and \"wget\" the \"contigs.zip\" file to restore it.  That's OK.  Things like this happen all the time.\n",
    "\n",
    "```\n",
    "$ ls -lh group12_contigs.fasta\n",
    "-rw-rw---- 1 kyclark staff 2.9M Aug 10 14:38 group12_contigs.fasta\n",
    "```\n",
    "\n",
    "Now that I have restored my data, I want to count how many greater-than signs in the file:\n",
    "\n",
    "```\n",
    "$ grep '>' group12_contigs.fasta | wc -l\n",
    "132\n",
    "```\n",
    "\n",
    "Hey, I could see doing that often.  Maybe we should make this into an \"alias\" \\(see above\\).  The problem is that the \"argument\" to the function \\(the filename\\) is stuck in the middle of the chain of commands, so it would make it tricky to use an alias for this.  We can create a bash function that we add to our .bashrc:\n",
    "\n",
    "```\n",
    "function countseqs() {\n",
    "  grep '>' $1 | wc -l\n",
    "}\n",
    "```\n",
    "\n",
    "After you add this, remember to source this file to make it available:\n",
    "\n",
    "```\n",
    "$ source ~/.bashrc\n",
    "$ countseqs group12_contigs.fasta\n",
    "132\n",
    "```\n",
    "\n",
    "Same answer.  Good.  However, someone beat us to the punch.  There is a powerful tool called \"seqmagick\" \\([https://github.com/fhcrc/seqmagick](https://github.com/fhcrc/seqmagick)\\) that will do this \\(and much, much more\\).  It's installed into the \"hurwitzlab/bin\" directory, or you can install it locally:\n",
    "\n",
    "```\n",
    "$ seqmagick info group12_contigs.fasta\n",
    "name                  alignment    min_len   max_len   avg_len  num_seqs\n",
    "group12_contigs.fasta FALSE           5136    116409  22974.30       132\n",
    "```\n",
    "\n",
    "Run \"seqmagick -h\" to see everything it can do.\n",
    "\n",
    "Moving on, let's find how many contig IDs in \"group12\\_contigs.fasta\" contain the number \"47\":\n",
    "\n",
    "```\n",
    "$ grep 47 group12_contigs.fasta > group12_ids_with_47\n",
    "[login3@~/work/sequences]$ cat !$\n",
    "cat group12_ids_with_47\n",
    ">Contig_247\n",
    ">Contig_447\n",
    ">Contig_476\n",
    ">Contig_1947\n",
    ">Contig_4764\n",
    ">Contig_4767\n",
    ">Contig_13471\n",
    "```\n",
    "\n",
    "Here we did a little \"useless use of cat,\" but it's OK.  We also could have used \"less\" to view the file.  Here's another useless use of cat to copy a file:\n",
    "\n",
    "```\n",
    "$ cat group12_ids_with_47 > temp1_ids\n",
    "```\n",
    "\n",
    "Additionally, we want to copy the file again to make duplicates:\n",
    "\n",
    "```\n",
    "$ cp group12_ids_with_47 temp2_ids\n",
    "```\n",
    "\n",
    "How can we be sure these files are the same?  Let's use \"diff\":\n",
    "\n",
    "```\n",
    "$ diff temp1_ids temp2_ids\n",
    "```\n",
    "\n",
    "You should see nothing, which is a case of \"no news is good news.\"  They don't differ in any way.  We can verify this with \"md5sum\":\n",
    "\n",
    "```\n",
    "$ md5sum temp*\n",
    "957390ab4c31db9500d148854f542eee  temp1_ids\n",
    "957390ab4c31db9500d148854f542eee  temp2_ids\n",
    "```\n",
    "\n",
    "They are the same file.  If there were even one character difference, they would generate different hashes.\n",
    "\n",
    "Now we will create a file with duplicate IDs:\n",
    "\n",
    "```\n",
    "$ cat temp1_ids temp2_ids > duplicate_ids\n",
    "```\n",
    "\n",
    "Check contents of \"duplicate\\_ids\" using \"less\" or \"cat.\"  Now grab all of the contigs IDs from \"group20\\_contigs.fasta\" that contain the number \"51.\"  Concatenate the new IDs to the duplicate\\_ids file in a file called \"multiple\\_ids\":\n",
    "\n",
    "```\n",
    "$ cp duplicate_ids multiple_ids\n",
    "$ grep 51 group20_contigs.fasta >> !$\n",
    "grep 51 group20_contigs.fasta >> multiple_ids\n",
    "```\n",
    "\n",
    "Notice the \"&gt;&gt;\" arrows to indicate that we are _appending_ to the existing \"multiple\\_ids\" file.\n",
    "\n",
    "Remove the existing \"temp\" files using a \"\\*\" wildcard:\n",
    "\n",
    "```\n",
    "$ rm temp*\n",
    "```\n",
    "\n",
    "Now let's explore more of what \"sort\" and \"uniq\" can do for us.  We want to find which IDs are unique and which are duplicated.  If we read the manpage \\(\"man uniq\"\\), we see that there are \"-d\" and \"-u\" flags for doing just that.  However, we've already seen that input to \"uniq\" needs to be sorted, so we need to remember to do that:\n",
    "\n",
    "```\n",
    "$ sort multiple_ids | uniq -d > temp1_ids\n",
    "$ sort multiple_ids | uniq -u > temp2_ids\n",
    "$ diff temp*\n",
    "1,7c1,11\n",
    "< >Contig_13471\n",
    "< >Contig_1947\n",
    "< >Contig_247\n",
    "< >Contig_447\n",
    "< >Contig_476\n",
    "< >Contig_4764\n",
    "< >Contig_4767\n",
    "---\n",
    "> >Contig_10051\n",
    "> >Contig_1651\n",
    "> >Contig_4851\n",
    "> >Contig_5141\n",
    "> >Contig_5143\n",
    "> >Contig_5164\n",
    "> >Contig_5170\n",
    "> >Contig_5188\n",
    "> >Contig_6351\n",
    "> >Contig_9651\n",
    "> >Contig_9851\n",
    "```\n",
    "\n",
    "Let's remove our temp files again and make a \"clean\\_ids\" file:\n",
    "\n",
    "```\n",
    "$ rm temp*\n",
    "$ sort multiple_ids | uniq > clean_ids\n",
    "$ wc -l multiple_ids clean_ids\n",
    " 25 multiple_ids\n",
    " 18 clean_ids\n",
    " 43 total\n",
    "```\n",
    "\n",
    "We can use \"sed\" to alter the IDs.  The \"s//\" command say to \"substitute\" the first thing with the second thing, e.g., to replace all occurences of \"foo\" with \"bar\", use \"s/foo/bar\" \\([http://stackoverflow.com/questions/4868904/what-is-the-origin-of-foo-and-bar](http://stackoverflow.com/questions/4868904/what-is-the-origin-of-foo-and-bar)\\).\n",
    "\n",
    "```\n",
    "$ sed 's/C/c/' clean_ids\n",
    "$ sed 's/_/./' clean_ids\n",
    "$ sed 's/>//' clean_ids > newclean_ids\n",
    "```\n",
    "\n",
    "That last one removes the FASTA file artifact that identifies the beginning of an ID but is not part of the ID.  We can use this with \"seqmagick\" now to extract those sequences and find out how many were found:\n",
    "\n",
    "```\n",
    "$ seqmagick convert --include-from-file newclean_ids group12_contigs.fasta newgroup12_contigs.fasta\n",
    "$ seqmagick info !$\n",
    "seqmagick info newgroup12_contigs.fasta\n",
    "name                     alignment    min_len   max_len   avg_len  num_seqs\n",
    "newgroup12_contigs.fasta FALSE           5587     30751  16768.14         7\n",
    "```\n",
    "\n",
    "We can get stats on all our files:\n",
    "\n",
    "```\n",
    "$ seqmagick info *fasta > fasta-info\n",
    "$ cat !$\n",
    "name                     alignment    min_len   max_len   avg_len  num_seqs\n",
    "group12_contigs.fasta    FALSE           5136    116409  22974.30       132\n",
    "group20_contigs.fasta    FALSE           5029     22601   7624.38       203\n",
    "group24_contigs.fasta    FALSE           5024     81329  12115.70       139\n",
    "newgroup12_contigs.fasta FALSE           5587     30751  16768.14         7\n",
    "```\n",
    "\n",
    "We can use \"cut\" to view various columns:\n",
    "\n",
    "```\n",
    "$ cut -f 2 fasta-info\n",
    "$ cut -f 2,4 fasta-info\n",
    "$ cut -f 2-4 fasta-info\n",
    "```\n",
    "\n",
    "But it does not line up very nicely.  We can use \"column\" to fix this:\n",
    "\n",
    "```\n",
    "$ cut -f 2-4 fasta-info | column -t\n",
    "alignment  min_len  max_len\n",
    "FALSE      5136     116409\n",
    "FALSE      5029     22601\n",
    "FALSE      5024     81329\n",
    "FALSE      5587     30751\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "df955ce39d0f31d56d4bb2fe0a613e5326ba60723fd33d8303a3aede8f65715c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
